{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO, Entrez\n",
    "import os\n",
    "from urllib.error import HTTPError\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import permutations, product\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_val_score\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pickle\n",
    "from os import path\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from warnings import simplefilter\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, balanced_accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "Entrez.tool = \"Zoonosis predictor\"\n",
    "\n",
    "Entrez.email = input(\"Enter an email address to use NCBI e-utils: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, name, X_test, y_test, params=None, dir='models/curr_models', gradBoost=False, xgBoost=False):\n",
    "    if not path.exists(f\"{dir}/{name}.pkl\"):\n",
    "        print(\"does not exist\")\n",
    "\n",
    "        pickle.dump(model, open(f'{dir}/{name}.pkl', 'wb'))\n",
    "    else:\n",
    "        predictions = model.predict(X_test)\n",
    "        currAcc = accuracy_score(y_test, predictions)\n",
    "\n",
    "        pickled_model = pickle.load(open(f'{dir}/{name}.pkl', 'rb'))\n",
    "        \n",
    "        if gradBoost:\n",
    "            # get features here \n",
    "            cols_when_model_builds = pickled_model.feature_names_in_\n",
    "            X_test=X_test[cols_when_model_builds]\n",
    "        elif xgBoost:\n",
    "            # put features into the same order that the model was trained in\n",
    "            cols_when_model_builds = pickled_model.get_booster().feature_names\n",
    "            X_test=X_test[cols_when_model_builds]\n",
    "        \n",
    "        # .values?\n",
    "        \n",
    "        picklePredictions=pickled_model.predict(X_test)\n",
    "        pickleAcc=accuracy_score(y_test, picklePredictions)\n",
    "        \n",
    "        if currAcc > pickleAcc:\n",
    "            print(\"update!\")\n",
    "\n",
    "            # TP, FP, FN, TN\n",
    "            print(confusion_matrix(y_test, picklePredictions).ravel())\n",
    "\n",
    "            print(\"curr\", currAcc, \"pickle\", pickleAcc)\n",
    "            pickle.dump(model, open(f'{dir}/{name}.pkl', 'wb'))\n",
    "\n",
    "            if params != None:\n",
    "                pickle.dump(params, open(f'{dir}/{name}-params.pkl', 'wb'))\n",
    "        else:\n",
    "            print(\"no update\")\n",
    "            print(\"curr\", currAcc, \"pickle\", pickleAcc)\n",
    "            \n",
    "            # TP, FP, FN, TN\n",
    "            print(confusion_matrix(y_test, picklePredictions).ravel())\n",
    "\n",
    "            model=pickled_model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetkmerdict(permset)->OrderedDict:\n",
    "        kmerdict = OrderedDict()\n",
    "        for i in permset:\n",
    "            kmerdict[i]=0\n",
    "        return kmerdict\n",
    "\n",
    "def assign_kmers_to_dict(st, permset, kmer):\n",
    "    kmerdict=resetkmerdict(permset)\n",
    "    # st = row[2] # tune for which column the sequence is in\n",
    "    for j in range(len(st)-kmer+1):\n",
    "        if not st[j:j+kmer] in permset: continue\n",
    "        kmerdict[st[j:j+kmer]]+=1\n",
    "    return kmerdict\n",
    "\n",
    "def getTrainParams(mergedDf, kmer):\n",
    "\n",
    "    s = product('acgt',repeat = kmer)\n",
    "    permset = set([\"\".join(x) for x in list(s)])\n",
    "    # print(permset)\n",
    "\n",
    "    l = []\n",
    "    \n",
    "    for row in tqdm.tqdm(mergedDf.itertuples()):\n",
    "        # print(row)\n",
    "        l.append(assign_kmers_to_dict(row[2], permset, kmer))\n",
    "\n",
    "    finalkmerdict=pd.DataFrame(l)\n",
    "    # print(finalkmerdict)\n",
    "\n",
    "    # print(\"finished\")\n",
    "    # mergedDf.fillna(0, inplace=True)\n",
    "\n",
    "    X = finalkmerdict\n",
    "    X = X.apply(lambda x: (x-x.min())/(x.max()-x.min()), axis=1)\n",
    "    Y = mergedDf['isZoonotic']\n",
    "\n",
    "    \n",
    "\n",
    "    # print(X)\n",
    "    # print(Y)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryKmer(ID, isZoonotic_list, index, everything):\n",
    "    FileName = \"{}.gb\".format(ID)\n",
    "    try:\n",
    "        QueryHandle = Entrez.efetch(db=\"nucleotide\", id=ID, \n",
    "                                    rettype=\"gb\", retmode=\"text\")\n",
    "    except HTTPError as Error:\n",
    "        if Error.code == 400:  # Bad request\n",
    "            raise ValueError(f\"Accession number not found: {ID}\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    SeqRec = SeqIO.read(QueryHandle, \"genbank\")\n",
    "    info = {'accession': ID, 'sequence': str(SeqRec.seq).lower(), 'isZoonotic': isZoonotic_list[index]}\n",
    "    everything.append(info)\n",
    "\n",
    "    pickle.dump(info, open(f\"sequences/{ID}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequences(mergedDf):\n",
    "    accession_list = [] # maintain order\n",
    "    isZoonotic_list = [] # maintain order\n",
    "    accession_set = set()\n",
    "    isZoonotic_set = set()\n",
    "\n",
    "\n",
    "    for row in tqdm.tqdm(mergedDf.itertuples()):\n",
    "        # row[13] = accession, row[15] = infects humans\n",
    "        for single_acc in row[14].split(\"; \"):\n",
    "            # print(single_acc)\n",
    "            if single_acc not in accession_set:\n",
    "                accession_list.append(single_acc)\n",
    "                isZoonotic_list.append(0 if not row[16] else 1)\n",
    "                # accession_set.add(single_acc)\n",
    "                # isZoonotic_set.add(row[15])\n",
    "                # print(0 if not row[16] else 1)\n",
    "\n",
    "    print(\"passed local retrieval\")\n",
    "    # TODO: RUN MULTIPLE THREADS TO SPEED UP\n",
    "    threads = []\n",
    "    vals = []\n",
    "\n",
    "    \n",
    "    for index, ID in enumerate(tqdm.tqdm(accession_list[:1000])): #only read the first 100 lol\n",
    "        # multithread for speed up\n",
    "        queryKmer(ID, isZoonotic_list, index, vals)\n",
    "        # x = threading.Thread(target=queryKmer, args=(ID, isZoonotic_list, index, vals))\n",
    "        # threads.append(x)\n",
    "        # x.start()\n",
    "\n",
    "    # for index, thread in enumerate(tqdm.tqdm(threads)):\n",
    "    #     thread.join()\n",
    "    df = pd.DataFrame(vals)\n",
    "    df.to_csv(\"data/nardus_sequences.csv\")\n",
    "\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FinalData_Cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/benjaminli/Documents/coding/scires/project/nardus.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benjaminli/Documents/coding/scires/project/nardus.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# SLOW METHOD\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/benjaminli/Documents/coding/scires/project/nardus.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mergedDf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mFinalData_Cleaned.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# read mollentze data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benjaminli/Documents/coding/scires/project/nardus.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sequences \u001b[39m=\u001b[39m getSequences(mergedDf)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benjaminli/Documents/coding/scires/project/nardus.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m (X_test, y_test) \u001b[39m=\u001b[39m getTrainParams(sequences)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seq/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seq/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seq/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seq/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seq/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seq/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/seq/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FinalData_Cleaned.csv'"
     ]
    }
   ],
   "source": [
    "# SLOW METHOD\n",
    "mergedDf = pd.read_csv(\"data/FinalData_Cleaned.csv\") # read mollentze data\n",
    "sequences = getSequences(mergedDf)\n",
    "(X_test, y_test) = getTrainParams(sequences, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "995    0\n",
      "996    0\n",
      "997    0\n",
      "998    0\n",
      "999    0\n",
      "Name: isZoonotic, Length: 1000, dtype: int64\n",
      "400\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print(sequences['isZoonotic'])\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gradBoost = pickle.load(open('models/curr_models/gradBoost.pkl', 'rb'))\n",
    "best_xgBoost = pickle.load(open('models/curr_models/xgBoost.pkl', 'rb'))\n",
    "\n",
    "kmer = 4\n",
    "s = product('acgt',repeat = kmer)\n",
    "permset = set([\"\".join(x) for x in list(s)])\n",
    "\n",
    "pred_arr = []\n",
    "xg_pred = []\n",
    "\n",
    "# blood validation\n",
    "for ind, file in enumerate(os.listdir(\"data/virome_contigs\")):\n",
    "    fasta_sequences = SeqIO.parse(open(f\"data/virome_contigs/{file}\"),'fasta')\n",
    "\n",
    "    fasta = [x for x in fasta_sequences][0]\n",
    "    # print(fasta)\n",
    "    \n",
    "    name, sequence = fasta.id, str(fasta.seq)\n",
    "    X_info = sequence.lower()\n",
    "\n",
    "    oDict = assign_kmers_to_dict(X_info, permset, kmer) # convert ordereddict to pandas dataframe\n",
    "\n",
    "    kmer_df = pd.DataFrame()\n",
    "\n",
    "    for i in oDict:\n",
    "        kmer_df.at[0, i]=oDict[i]\n",
    "\n",
    "    kmer_df = kmer_df.apply(lambda x: (x-x.min())/(x.max()-x.min()), axis=1)\n",
    "    \n",
    "    cols_when_model_builds = best_gradBoost.feature_names_in_\n",
    "    kmer_df=kmer_df[cols_when_model_builds]\n",
    "    \n",
    "    # print(kmer_df.to_string())\n",
    "    \n",
    "    pred_arr.append(best_gradBoost.predict(kmer_df))\n",
    "    \n",
    "    cols_when_model_builds = best_xgBoost.get_booster().feature_names\n",
    "    kmer_df=kmer_df[cols_when_model_builds]\n",
    "    # print(kmer_df.to_string())\n",
    "\n",
    "    xg_pred.append(best_xgBoost.predict(kmer_df))\n",
    "    \n",
    "pred_arr = np.asarray(pred_arr)\n",
    "xg_pred = np.asarray(xg_pred)\n",
    "\n",
    "\n",
    "    # print(best_gradBoost.predict(kmer_df), sequences.loc[ind]['isZoonotic'])\n",
    "    # print(best_gradBoost.predict(kmer_df), sequences['isZoonotic'])\n",
    "        # print(accuracy_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       accession                                           sequence  \\\n",
      "0    NC_018629.1  acgcttaacagctaaaaaccagaagacagagaaggaatcgaagggg...   \n",
      "1    NC_006507.1  agcaaaaacaggcagtcaaaatggctacagaccagatggacatctc...   \n",
      "2    NC_004119.1  agttggttttgccggctacaacgatcctccgtaggaagcgttggtg...   \n",
      "3     AF075254.1  atgggcggcgtatgagagaagcccaaaacctagactacccataatg...   \n",
      "4    NC_012533.1  acttttttgatcggtgtgaatcgatacaaacagattttgtagagaa...   \n",
      "..           ...                                                ...   \n",
      "995  NC_001693.1  tataggatatacactgattgttggcaactatcattaaatcataaaa...   \n",
      "996  NC_030117.1  ggctcaagcctctcgcggctgtgcaagtggaaaaaaaattttttta...   \n",
      "997  NC_018481.1  cgcactggggatccgagttaaagtatcaatcttccgtgtataatct...   \n",
      "998   KP792660.1  taaagcaatggcgcatcaacttggagagatagtgaggcagtttgct...   \n",
      "999   KU925448.1  atgtctgtcttagactttaccgacatccgtggtctgaatgattggt...   \n",
      "\n",
      "     isZoonotic  \n",
      "0             1  \n",
      "1             1  \n",
      "2             0  \n",
      "3             1  \n",
      "4             0  \n",
      "..          ...  \n",
      "995           1  \n",
      "996           0  \n",
      "997           0  \n",
      "998           1  \n",
      "999           0  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:05, 185.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# FAST METHOD - LOAD DATA FROM FOLDER\n",
    "li = []\n",
    "for f in os.listdir('data/sequences'):\n",
    "    seqdf = pickle.load(open(f'data/sequences/{f}', 'rb'))\n",
    "    li.append(seqdf)\n",
    "df = pd.DataFrame(li)\n",
    "print(df)\n",
    "\n",
    "# print(df)\n",
    "(X_test_temp, y_test_temp) = getTrainParams(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_temp), len(y_test_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0.644\n",
      "[523  17 339 121]\n",
      "0.651\n",
      "[523  17 339 121]\n"
     ]
    }
   ],
   "source": [
    "cols_when_model_builds = best_gradBoost.feature_names_in_\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "print(len(X_test_temp))\n",
    "print(accuracy_score(y_test_temp, best_gradBoost.predict(X_test_temp)))\n",
    "print(confusion_matrix(y_test_temp, best_gradBoost.predict(X_test_temp)).ravel())\n",
    "\n",
    "# print(y_test.to_string())\n",
    "\n",
    "cols_when_model_builds = best_xgBoost.get_booster().feature_names\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "print(accuracy_score(y_test_temp, best_xgBoost.predict(X_test_temp)))\n",
    "print(confusion_matrix(y_test_temp, best_gradBoost.predict(X_test_temp)).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tggc      ttcc      tatg      gtat      aatg      acag      atgg  \\\n",
      "0    0.129032  0.296774  0.341935  0.193548  0.406452  0.393548  0.412903   \n",
      "1    0.315789  0.263158  0.105263  0.105263  0.473684  0.473684  0.526316   \n",
      "2    0.540146  0.218978  0.240876  0.094891  0.693431  0.335766  0.715328   \n",
      "3    0.451220  0.353659  0.560976  0.402439  0.475610  0.658537  0.378049   \n",
      "4    0.547619  0.198413  0.134921  0.071429  0.420635  0.634921  0.658730   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "995  0.181818  0.313131  0.393939  0.313131  0.626263  0.535354  0.313131   \n",
      "996  0.063178  0.157752  0.276744  0.219767  0.310078  0.110078  0.229070   \n",
      "997  0.280000  0.240000  0.160000  0.160000  0.340000  0.220000  0.260000   \n",
      "998  0.226804  0.144330  0.453608  0.247423  0.587629  0.340206  0.422680   \n",
      "999  0.550000  0.250000  0.300000  0.150000  0.800000  0.800000  0.500000   \n",
      "\n",
      "         tact      tcaa      aggc  ...      ggca      gacc      agct  \\\n",
      "0    0.335484  0.587097  0.200000  ...  0.219355  0.219355  0.296774   \n",
      "1    0.157895  0.684211  0.368421  ...  0.578947  0.684211  0.210526   \n",
      "2    0.204380  0.430657  0.182482  ...  0.262774  0.175182  0.240876   \n",
      "3    0.341463  0.390244  0.390244  ...  0.524390  0.402439  0.390244   \n",
      "4    0.055556  0.261905  0.404762  ...  0.452381  0.404762  0.444444   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "995  0.313131  0.272727  0.151515  ...  0.171717  0.202020  0.333333   \n",
      "996  0.196512  0.298837  0.036822  ...  0.030233  0.044186  0.138760   \n",
      "997  0.140000  0.580000  0.180000  ...  0.120000  0.140000  0.160000   \n",
      "998  0.309278  0.556701  0.175258  ...  0.257732  0.195876  0.216495   \n",
      "999  0.050000  0.500000  0.500000  ...  0.500000  0.200000  0.550000   \n",
      "\n",
      "         agta      tatc      acgt      cact      cttg      ctat      tctt  \n",
      "0    0.187097  0.361290  0.058065  0.245161  0.335484  0.322581  0.625806  \n",
      "1    0.157895  0.157895  0.052632  0.368421  0.263158  0.105263  0.157895  \n",
      "2    0.116788  0.153285  0.065693  0.335766  0.467153  0.197080  0.321168  \n",
      "3    0.414634  0.170732  0.329268  0.268293  0.329268  0.365854  0.109756  \n",
      "4    0.071429  0.111111  0.111111  0.230159  0.428571  0.142857  0.190476  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "995  0.373737  0.222222  0.121212  0.161616  0.212121  0.393939  0.343434  \n",
      "996  0.178295  0.238372  0.080233  0.087597  0.116279  0.274419  0.260078  \n",
      "997  0.120000  0.180000  0.020000  0.320000  0.300000  0.140000  0.740000  \n",
      "998  0.247423  0.329897  0.041237  0.185567  0.195876  0.360825  0.340206  \n",
      "999  0.250000  0.050000  0.000000  0.250000  0.650000  0.250000  0.250000  \n",
      "\n",
      "[1000 rows x 256 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv('data/info.csv')\n",
    "\n",
    "X_vals = pd.concat([orig_df.loc[:, orig_df.columns != 'isZoonotic'], X_test_temp], axis=0, ignore_index=True)\n",
    "\n",
    "# # print(X_vals)\n",
    "y_vals = pd.concat([orig_df['isZoonotic'], y_test_temp], axis=0, ignore_index=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vals, y_vals, test_size=0.2, random_state=1)\n",
    "\n",
    "# print(len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10430\n",
      "Learning rate:  0.05\n",
      "Accuracy score (validation): 0.914\n",
      "no update\n",
      "curr 0.914189837008629 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.075\n",
      "Accuracy score (validation): 0.919\n",
      "no update\n",
      "curr 0.9189837008628955 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.1\n",
      "Accuracy score (validation): 0.919\n",
      "no update\n",
      "curr 0.9189837008628955 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.15\n",
      "Accuracy score (validation): 0.926\n",
      "no update\n",
      "curr 0.9261744966442953 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.2\n",
      "Accuracy score (validation): 0.926\n",
      "no update\n",
      "curr 0.9256951102588686 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.25\n",
      "Accuracy score (validation): 0.929\n",
      "no update\n",
      "curr 0.9290508149568553 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.3\n",
      "Accuracy score (validation): 0.926\n",
      "no update\n",
      "curr 0.9256951102588686 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.35\n",
      "Accuracy score (validation): 0.922\n",
      "no update\n",
      "curr 0.9223394055608821 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.4\n",
      "Accuracy score (validation): 0.926\n",
      "no update\n",
      "curr 0.9256951102588686 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.45\n",
      "Accuracy score (validation): 0.926\n",
      "no update\n",
      "curr 0.9256951102588686 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.5\n",
      "Accuracy score (validation): 0.923\n",
      "no update\n",
      "curr 0.9232981783317353 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.55\n",
      "Accuracy score (validation): 0.916\n",
      "no update\n",
      "curr 0.9156279961649089 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.6\n",
      "Accuracy score (validation): 0.919\n",
      "no update\n",
      "curr 0.9194630872483222 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.65\n",
      "Accuracy score (validation): 0.894\n",
      "no update\n",
      "curr 0.8940556088207094 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.7\n",
      "Accuracy score (validation): 0.882\n",
      "no update\n",
      "curr 0.8820709491850431 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.75\n",
      "Accuracy score (validation): 0.888\n",
      "no update\n",
      "curr 0.887823585810163 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.8\n",
      "Accuracy score (validation): 0.838\n",
      "no update\n",
      "curr 0.8384467881112176 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  0.85\n",
      "Accuracy score (validation): 0.861\n",
      "no update\n",
      "curr 0.861457334611697 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n",
      "Learning rate:  1\n",
      "Accuracy score (validation): 0.838\n",
      "no update\n",
      "curr 0.8384467881112176 pickle 0.9290508149568553\n",
      "[1685   28  120  253]\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65,0.7, 0.75, 0.8, 0.85, 1]\n",
    "\n",
    "parameters={\n",
    "   \"n_estimators\":120, # 200 kind of overfits I think\n",
    "    \"max_features\":2,\n",
    "    \"max_depth\":6,\n",
    "    \"random_state\":0,\n",
    "    \"min_sample_split\":50,\n",
    "    \"subsample\":0.8\n",
    "    # \"warm_start\":True\n",
    "}\n",
    "\n",
    "\n",
    "# careful with WARM START - only works after a lot of iterations\n",
    "# integrating both datasets works better!\n",
    "\n",
    "print(len(X_train)+len(X_test))\n",
    "for learning_rate in lr_list:\n",
    "    gradBoost = GradientBoostingClassifier(n_estimators=parameters['n_estimators'], \n",
    "    learning_rate=learning_rate, max_features=parameters['max_features'], \n",
    "    max_depth=parameters['max_depth'], random_state=parameters['random_state'], \n",
    "    min_samples_split=parameters['min_sample_split'], subsample=parameters['subsample']\n",
    "    )\n",
    "\n",
    "    parameters['learning_rate']=learning_rate\n",
    "    gradBoost.fit(X_train, y_train)\n",
    "\n",
    "    cols_when_model_builds = gradBoost.feature_names_in_\n",
    "    X_test=X_test[cols_when_model_builds]\n",
    "    \n",
    "    testingAcc = accuracy_score(y_test, gradBoost.predict(X_test))\n",
    "    trainingAcc = accuracy_score(y_train, gradBoost.predict(X_train))\n",
    "    \n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    # print(\"Accuracy score (training): {0:.3f}\".format(trainingAcc))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(testingAcc))\n",
    "    # print(f\"Feature importance {gradBoost.feature_importances_}\")\n",
    "\n",
    "    # pickle.dump(gradBoost, open('gradBoost.pkl', 'wb'))\n",
    "    saveModel(gradBoost, \"gradBoost\", X_test, y_test, parameters, gradBoost=True, dir=\"models/nardus_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n",
      "[529  11  79 381]\n"
     ]
    }
   ],
   "source": [
    "gradBoost = pickle.load(open('models/nardus_testing/gradBoost.pkl', 'rb'))\n",
    "cols_when_model_builds = gradBoost.feature_names_in_\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "\n",
    "print(accuracy_score(y_test_temp, gradBoost.predict(X_test_temp)))\n",
    "print(confusion_matrix(y_test_temp, gradBoost.predict(X_test_temp)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6912 candidates, totalling 34560 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(learning_rate=0.05,\n",
       "                                                  max_depth=8, max_features=2,\n",
       "                                                  min_samples_split=60,\n",
       "                                                  n_estimators=150,\n",
       "                                                  random_state=0,\n",
       "                                                  subsample=0.8),\n",
       "             n_jobs=4,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.15, 0.2, 0.25, 0.3, 0.35],\n",
       "                         &#x27;max_depth&#x27;: range(4, 10),\n",
       "                         &#x27;min_samples_split&#x27;: range(10, 50, 5),\n",
       "                         &#x27;n_estimators&#x27;: range(20, 80, 10),\n",
       "                         &#x27;subsample&#x27;: [0.75, 0.8, 0.85, 0.9]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(learning_rate=0.05,\n",
       "                                                  max_depth=8, max_features=2,\n",
       "                                                  min_samples_split=60,\n",
       "                                                  n_estimators=150,\n",
       "                                                  random_state=0,\n",
       "                                                  subsample=0.8),\n",
       "             n_jobs=4,\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.15, 0.2, 0.25, 0.3, 0.35],\n",
       "                         &#x27;max_depth&#x27;: range(4, 10),\n",
       "                         &#x27;min_samples_split&#x27;: range(10, 50, 5),\n",
       "                         &#x27;n_estimators&#x27;: range(20, 80, 10),\n",
       "                         &#x27;subsample&#x27;: [0.75, 0.8, 0.85, 0.9]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.05, max_depth=8, max_features=2,\n",
       "                           min_samples_split=60, n_estimators=150,\n",
       "                           random_state=0, subsample=0.8)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.05, max_depth=8, max_features=2,\n",
       "                           min_samples_split=60, n_estimators=150,\n",
       "                           random_state=0, subsample=0.8)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(learning_rate=0.05,\n",
       "                                                  max_depth=8, max_features=2,\n",
       "                                                  min_samples_split=60,\n",
       "                                                  n_estimators=150,\n",
       "                                                  random_state=0,\n",
       "                                                  subsample=0.8),\n",
       "             n_jobs=4,\n",
       "             param_grid={'learning_rate': [0.1, 0.15, 0.2, 0.25, 0.3, 0.35],\n",
       "                         'max_depth': range(4, 10),\n",
       "                         'min_samples_split': range(10, 50, 5),\n",
       "                         'n_estimators': range(20, 80, 10),\n",
       "                         'subsample': [0.75, 0.8, 0.85, 0.9]},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr_list = [0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65,0.7, 0.75, 0.8, 0.85, 1]\n",
    "\n",
    "# parameters={\n",
    "#    \"n_estimators\":150, # 200 kind of overfits I think\n",
    "#     \"max_features\":2,\n",
    "#     \"max_depth\":8,\n",
    "#     \"random_state\":0,\n",
    "#     \"min_sample_split\":60,\n",
    "#     \"subsample\":0.8\n",
    "#     # \"warm_start\":True\n",
    "# }\n",
    "\n",
    "\n",
    "# # careful with WARM START - only works after a lot of iterations\n",
    "# # for learning_rate in lr_list:\n",
    "\n",
    "# # gradBoost = GradientBoostingClassifier(n_estimators=parameters['n_estimators'], \n",
    "# # learning_rate=learning_rate, max_features=parameters['max_features'], \n",
    "# # max_depth=parameters['max_depth'], random_state=parameters['random_state'], \n",
    "# # min_samples_split=parameters['min_sample_split'], subsample=parameters['subsample']\n",
    "# # )\n",
    "# learning_rate = 0.05\n",
    "\n",
    "# param_test1 = {'n_estimators':range(20,80,10), 'learning_rate':[0.1,0.15,0.2,0.25,0.3,0.35], 'subsample':[0.75,0.8,0.85,0.9], 'max_depth':range(4,10,1), 'min_samples_split':range(10,50,5)}\n",
    "\n",
    "# gradBoost = GridSearchCV(estimator = GradientBoostingClassifier(\n",
    "#     n_estimators=parameters['n_estimators'], \n",
    "# learning_rate=learning_rate, max_features=parameters['max_features'], \n",
    "# max_depth=parameters['max_depth'], random_state=parameters['random_state'], \n",
    "# min_samples_split=parameters['min_sample_split'], subsample=parameters['subsample']\n",
    "# ), \n",
    "# param_grid = param_test1, scoring='roc_auc',n_jobs=4, cv=5, verbose=1)\n",
    "\n",
    "\n",
    "# parameters['learning_rate']=learning_rate\n",
    "# gradBoost.fit(X_train, y_train)\n",
    "\n",
    "# # cols_when_model_builds = gradBoost.feature_names_in_\n",
    "# # X_test=X_test[cols_when_model_builds]\n",
    "\n",
    "# # testingAcc = accuracy_score(y_test, gradBoost.predict(X_test))\n",
    "# # trainingAcc = accuracy_score(y_train, gradBoost.predict(X_train))\n",
    "\n",
    "# # print(\"Learning rate: \", learning_rate)\n",
    "# # # print(\"Accuracy score (training): {0:.3f}\".format(trainingAcc))\n",
    "# # print(\"Accuracy score (validation): {0:.3f}\".format(testingAcc))\n",
    "# # print(f\"Feature importance {gradBoost.feature_importances_}\")\n",
    "\n",
    "# # pickle.dump(gradBoost, open('gradBoost.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.3, 'max_depth': 7, 'min_samples_split': 15, 'n_estimators': 70, 'subsample': 0.75} 0.6914823529411764\n"
     ]
    }
   ],
   "source": [
    "print(gradBoost.best_params_, gradBoost.best_score_)\n",
    "\n",
    "pickle.dump(gradBoost.best_estimator_, open('gridsearch/gradBoost.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.935]\n",
      "[503  37  28 432]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test_temp, gradBoost.best_estimator_.predict(X_test_temp)).ravel())\n",
    "\n",
    "print(confusion_matrix(y_test_temp, gradBoost.best_estimator_.predict(X_test_temp)).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.05\n",
      "Accuracy score (validation): 0.630\n",
      "Learning rate:  0.075\n",
      "Accuracy score (validation): 0.650\n",
      "Learning rate:  0.1\n",
      "Accuracy score (validation): 0.630\n",
      "Learning rate:  0.15\n",
      "Accuracy score (validation): 0.660\n",
      "Learning rate:  0.2\n",
      "Accuracy score (validation): 0.640\n",
      "Learning rate:  0.25\n",
      "Accuracy score (validation): 0.640\n",
      "Learning rate:  0.3\n",
      "Accuracy score (validation): 0.630\n",
      "Learning rate:  0.35\n",
      "Accuracy score (validation): 0.640\n",
      "Learning rate:  0.4\n",
      "Accuracy score (validation): 0.630\n",
      "Learning rate:  0.45\n",
      "Accuracy score (validation): 0.645\n",
      "Learning rate:  0.5\n",
      "Accuracy score (validation): 0.635\n",
      "Learning rate:  0.55\n",
      "Accuracy score (validation): 0.625\n",
      "Learning rate:  0.6\n",
      "Accuracy score (validation): 0.635\n",
      "Learning rate:  0.65\n",
      "Accuracy score (validation): 0.595\n",
      "Learning rate:  0.7\n",
      "Accuracy score (validation): 0.630\n",
      "Learning rate:  0.75\n",
      "Accuracy score (validation): 0.635\n",
      "Learning rate:  0.8\n",
      "Accuracy score (validation): 0.610\n",
      "Learning rate:  0.85\n",
      "Accuracy score (validation): 0.625\n",
      "Learning rate:  1\n",
      "Accuracy score (validation): 0.580\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65,0.7, 0.75, 0.8, 0.85, 1]\n",
    "\n",
    "parameters={\n",
    "   \"n_estimators\":100, # 200 kind of overfits I think\n",
    "    \"max_features\":2,\n",
    "    \"max_depth\":8,\n",
    "    \"random_state\":0,\n",
    "    \"subsample\":0.8,\n",
    "    'lambda': 0.5, # regularization?\n",
    "    'alpha': 0.5\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    xgBoost = XGBClassifier(n_estimators=parameters['n_estimators'], \n",
    "    learning_rate=learning_rate, \n",
    "    max_depth=parameters['max_depth'], random_state=parameters['random_state'], \n",
    "    subsample=parameters['subsample'], \n",
    "    )\n",
    "\n",
    "    parameters['learning_rate']=learning_rate\n",
    "    xgBoost.fit(X_train, y_train)\n",
    "\n",
    "    # ALWAYS reset feature names\n",
    "    cols_when_model_builds = xgBoost.get_booster().feature_names\n",
    "    X_test=X_test[cols_when_model_builds]\n",
    "\n",
    "    testingAcc = accuracy_score(y_test, xgBoost.predict(X_test))\n",
    "    trainingAcc = accuracy_score(y_train, xgBoost.predict(X_train))\n",
    "    \n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    # print(\"Accuracy score (training): {0:.3f}\".format(trainingAcc))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(testingAcc))\n",
    "    # print(f\"Feature importance {gradBoost.feature_importances_}\")\n",
    "\n",
    "    # pickle.dump(gradBoost, open('gradBoost.pkl', 'wb'))\n",
    "    # saveModel(xgBoost, \"xgBoost\", X_test, y_test, parameters, xgBoost=True, dir=\"synthetic_data_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62375\n"
     ]
    }
   ],
   "source": [
    "randforest = pickle.load(open('curr_models/randforest.pkl', 'rb'))\n",
    "print(accuracy_score(y_train, randforest.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67\n"
     ]
    }
   ],
   "source": [
    "knn = pickle.load(open('curr_models/knn.pkl', 'rb'))\n",
    "\n",
    "print(accuracy_score(y_test, knn.predict(X_test)))\n",
    "# knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: PA544053.1\n",
      "Name: PA544053\n",
      "Description: WO 2022071435-A/1: SARS-CoV-2 PROTEIN-DERIVED PEPTIDES AND VACCINES INCLUDING THE SAME\n",
      "Number of features: 1\n",
      "/molecule_type=DNA\n",
      "/topology=linear\n",
      "/data_file_division=PAT\n",
      "/date=29-JUL-2022\n",
      "/accessions=['PA544053']\n",
      "/sequence_version=1\n",
      "/keywords=['WO 2022071435-A/1']\n",
      "/source=Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)\n",
      "/organism=Severe acute respiratory syndrome coronavirus 2\n",
      "/taxonomy=['Viruses', 'Riboviria', 'Orthornavirae', 'Pisuviricota', 'Pisoniviricetes', 'Nidovirales', 'Cornidovirineae', 'Coronaviridae', 'Orthocoronavirinae', 'Betacoronavirus', 'Sarbecovirus']\n",
      "/references=[Reference(title='SARS-CoV-2 PROTEIN-DERIVED PEPTIDES AND VACCINES INCLUDING THE SAME', ...)]\n",
      "/comment=OS   Severe acute respiratory syndrome coronavirus 2 PN   WO\n",
      "2022071435-A/1\n",
      "PD   07-APR-2022\n",
      "PF   29-SEP-2021 WO 2021JP035967\n",
      "PR   30-SEP-2020 JP 2020-164630         ,30-APR-2021 JP\n",
      "PR   JP2021/017159       ,\n",
      "PR   25-AUG-2021 US 63/236927\n",
      "PA   ONCOTHERAPY SCIENCE INC,CANCER PRECISION MEDICINE INC,Kazuma\n",
      "PA   KIYOTANI,\n",
      "PA   Yusuke NAKAMURA\n",
      "PI   tetsuro hikichi,yusuke nakamura,kazuma kiyotani\n",
      "PT   'SARS-CoV-2 PROTEIN-DERIVED PEPTIDES AND VACCINES INCLUDING\n",
      "PT   THE SAME'\n",
      "PS   N16\n",
      "CC\n",
      "FH   Key             Location/Qualifiers.\n",
      "Seq('ATTAAAGGTTTATACCTTCCCAGGTAACAAACCAACCAACTTTCGATCTCTTGT...AAA')\n",
      "[0.33, 0.67]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def getSingleSequence(accessionID):\n",
    "    try:\n",
    "        QueryHandle = Entrez.efetch(db=\"nucleotide\", id=accessionID, \n",
    "                                    rettype=\"gb\", retmode=\"text\")\n",
    "    except HTTPError as Error:\n",
    "        if Error.code == 400:  # Bad request\n",
    "            raise ValueError(f\"Accession number not found: {accessionID}\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    SeqRec = SeqIO.read(QueryHandle, \"genbank\")\n",
    "    print(str(SeqRec))\n",
    "\n",
    "    oDict = assign_kmers_to_dict(X_info, permset, kmer) # convert ordereddict to pandas dataframe\n",
    "\n",
    "    kmer_df = pd.DataFrame()\n",
    "\n",
    "    for i in oDict:\n",
    "        kmer_df.at[0, i]=oDict[i]\n",
    "    # print(best_gradBoost.predict_proba(kmer_df))\n",
    "\n",
    "    cols_when_model_builds = best_gradBoost.feature_names_in_\n",
    "    kmer_df=kmer_df[cols_when_model_builds]\n",
    "    \n",
    "    print([round(x, 2) for x in best_gradBoost.predict_proba(kmer_df).tolist()[0]])\n",
    "    print(best_gradBoost.predict(kmer_df).tolist()[0])\n",
    "\n",
    "    # ls = []\n",
    "    # ls.append({'accession': accessionID, 'sequence': str(SeqRec.seq).lower()})\n",
    "    # df = pd.DataFrame(ls)\n",
    "    # print(df)\n",
    "\n",
    "getSingleSequence(\"PA544053\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('seq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1360c63304de9435a2a3572d38e6c9496b6fb5d1617f35fbc8638664d664ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
