{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO, Entrez\n",
    "import os\n",
    "from urllib.error import HTTPError\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import permutations, product\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_val_score\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pickle\n",
    "from os import path\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from warnings import simplefilter\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, balanced_accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "Entrez.tool = \"Zoonosis predictor\"\n",
    "\n",
    "Entrez.email = input(\"Enter an email address to use NCBI e-utils: \")\n",
    "\n",
    "# in order to import model utils and validation utils\n",
    "if (os.path.abspath('').split('/')[-1] == 'project'):\n",
    "    %cd utils\n",
    "elif (os.path.abspath('').split('/')[-1] == 'train_and_vis'):\n",
    "    %cd ../utils\n",
    "\n",
    "import query_utils\n",
    "import model_utils\n",
    "import validation_utils\n",
    "\n",
    "if (os.path.abspath('').split('/')[-1] == 'utils'):\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetkmerdict(permset)->OrderedDict:\n",
    "        kmerdict = OrderedDict()\n",
    "        for i in permset:\n",
    "            kmerdict[i]=0\n",
    "        return kmerdict\n",
    "\n",
    "def assign_kmers_to_dict(st, permset, kmer):\n",
    "    kmerdict=resetkmerdict(permset)\n",
    "    # st = row[2] # tune for which column the sequence is in\n",
    "    for j in range(len(st)-kmer+1):\n",
    "        if not st[j:j+kmer] in permset: continue\n",
    "        kmerdict[st[j:j+kmer]]+=1\n",
    "    return kmerdict\n",
    "\n",
    "def getTrainParams(mergedDf, kmer):\n",
    "\n",
    "    s = product('acgt',repeat = kmer)\n",
    "    permset = set([\"\".join(x) for x in list(s)])\n",
    "    # print(permset)\n",
    "\n",
    "    l = []\n",
    "    \n",
    "    for row in tqdm.tqdm(mergedDf.itertuples()):\n",
    "        # print(row)\n",
    "        l.append(assign_kmers_to_dict(row[1], permset, kmer))\n",
    "\n",
    "    finalkmerdict=pd.DataFrame(l)\n",
    "    # print(finalkmerdict)\n",
    "\n",
    "    # print(\"finished\")\n",
    "    # mergedDf.fillna(0, inplace=True)\n",
    "\n",
    "    X = finalkmerdict\n",
    "    X = X.apply(lambda x: (x-x.min())/(x.max()-x.min()), axis=1)\n",
    "    Y = mergedDf['isZoonotic']\n",
    "\n",
    "    \n",
    "\n",
    "    # print(X)\n",
    "    # print(Y)\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryKmer(ID, isZoonotic_list, index, everything):\n",
    "    FileName = \"{}.gb\".format(ID)\n",
    "    try:\n",
    "        QueryHandle = Entrez.efetch(db=\"nucleotide\", id=ID, \n",
    "                                    rettype=\"gb\", retmode=\"text\")\n",
    "    except HTTPError as Error:\n",
    "        if Error.code == 400:  # Bad request\n",
    "            raise ValueError(f\"Accession number not found: {ID}\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    SeqRec = SeqIO.read(QueryHandle, \"genbank\")\n",
    "    print(SeqRec.name)\n",
    "    info = {'ID': ID, 'DNA Sequence': str(SeqRec.seq), 'isZoonotic': isZoonotic_list[index]}\n",
    "    everything.append(info)\n",
    "\n",
    "    pickle.dump(info, open(f\"data/sequences/{ID}.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSequences(mergedDf):\n",
    "    accession_list = [] # maintain order\n",
    "    isZoonotic_list = [] # maintain order\n",
    "    accession_set = set()\n",
    "    isZoonotic_set = set()\n",
    "\n",
    "\n",
    "    for row in tqdm.tqdm(mergedDf.itertuples()):\n",
    "        # row[13] = accession, row[15] = infects humans\n",
    "        for single_acc in row[14].split(\"; \"):\n",
    "            # print(single_acc)\n",
    "            if single_acc not in accession_set:\n",
    "                accession_list.append(single_acc)\n",
    "                isZoonotic_list.append(0 if not row[16] else 1)\n",
    "                # accession_set.add(single_acc)\n",
    "                # isZoonotic_set.add(row[15])\n",
    "                # print(0 if not row[16] else 1)\n",
    "\n",
    "    print(\"passed local retrieval\")\n",
    "    # TODO: RUN MULTIPLE THREADS TO SPEED UP\n",
    "    threads = []\n",
    "    vals = []\n",
    "\n",
    "    print(len(accession_list))\n",
    "    for index, ID in enumerate(tqdm.tqdm(accession_list)): #only read the first 100 lol\n",
    "        # multithread for speed up\n",
    "        queryKmer(ID, isZoonotic_list, index, vals)\n",
    "        # x = threading.Thread(target=queryKmer, args=(ID, isZoonotic_list, index, vals))\n",
    "        # threads.append(x)\n",
    "        # x.start()\n",
    "\n",
    "    # for index, thread in enumerate(tqdm.tqdm(threads)):\n",
    "    #     thread.join()\n",
    "    df = pd.DataFrame(vals)\n",
    "    df.to_csv(\"data/nardus_sequences.csv\", index=False)\n",
    "\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOW METHOD\n",
    "mergedDf = pd.read_csv(\"data/FinalData_Cleaned.csv\")\n",
    "print(len(mergedDf))\n",
    "sequences = getSequences(mergedDf)\n",
    "(X_test, y_test) = getTrainParams(sequences, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST METHOD - LOAD DATA FROM FOLDER\n",
    "li = []\n",
    "for f in os.listdir('data/sequences'):\n",
    "    seqdf = pickle.load(open(f'data/sequences/{f}', 'rb'))\n",
    "    li.append(seqdf)\n",
    "df = pd.DataFrame(li)\n",
    "print(df)\n",
    "\n",
    "# print(df)\n",
    "(X_test_temp, y_test_temp) = getTrainParams(df, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gradBoost = pickle.load(open('models/curr_models/gradBoost.pkl', 'rb'))\n",
    "best_xgBoost = pickle.load(open('models/curr_models/xgBoost.pkl', 'rb'))\n",
    "\n",
    "kmer = 4\n",
    "s = product('acgt',repeat = kmer)\n",
    "permset = set([\"\".join(x) for x in list(s)])\n",
    "\n",
    "pred_arr = []\n",
    "xg_pred = []\n",
    "\n",
    "# blood validation\n",
    "for ind, file in enumerate(os.listdir(\"data/virome_contigs\")):\n",
    "    fasta_sequences = SeqIO.parse(open(f\"data/virome_contigs/{file}\"),'fasta')\n",
    "\n",
    "    fasta = [x for x in fasta_sequences][0]\n",
    "    # print(fasta)\n",
    "    \n",
    "    name, sequence = fasta.id, str(fasta.seq)\n",
    "    X_info = sequence.lower()\n",
    "\n",
    "    oDict = assign_kmers_to_dict(X_info, permset, kmer) # convert ordereddict to pandas dataframe\n",
    "\n",
    "    kmer_df = pd.DataFrame()\n",
    "\n",
    "    for i in oDict:\n",
    "        kmer_df.at[0, i]=oDict[i]\n",
    "\n",
    "    kmer_df = kmer_df.apply(lambda x: (x-x.min())/(x.max()-x.min()), axis=1)\n",
    "    \n",
    "    cols_when_model_builds = best_gradBoost.feature_names_in_\n",
    "    kmer_df=kmer_df[cols_when_model_builds]\n",
    "    \n",
    "    # print(kmer_df.to_string())\n",
    "    \n",
    "    pred_arr.append(best_gradBoost.predict(kmer_df))\n",
    "    \n",
    "    cols_when_model_builds = best_xgBoost.get_booster().feature_names\n",
    "    kmer_df=kmer_df[cols_when_model_builds]\n",
    "    # print(kmer_df.to_string())\n",
    "\n",
    "    xg_pred.append(best_xgBoost.predict(kmer_df))\n",
    "    \n",
    "pred_arr = np.asarray(pred_arr)\n",
    "xg_pred = np.asarray(xg_pred)\n",
    "\n",
    "\n",
    "    # print(best_gradBoost.predict(kmer_df), sequences.loc[ind]['isZoonotic'])\n",
    "    # print(best_gradBoost.predict(kmer_df), sequences['isZoonotic'])\n",
    "        # print(accuracy_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test_temp), len(y_test_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_when_model_builds = best_gradBoost.feature_names_in_\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "print(len(X_test_temp))\n",
    "print(accuracy_score(y_test_temp, best_gradBoost.predict(X_test_temp)))\n",
    "print(confusion_matrix(y_test_temp, best_gradBoost.predict(X_test_temp)).ravel())\n",
    "\n",
    "# print(y_test.to_string())\n",
    "\n",
    "cols_when_model_builds = best_xgBoost.get_booster().feature_names\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "print(accuracy_score(y_test_temp, best_xgBoost.predict(X_test_temp)))\n",
    "print(confusion_matrix(y_test_temp, best_gradBoost.predict(X_test_temp)).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv('data/info.csv')\n",
    "X_vals = pd.concat([orig_df.loc[:, orig_df.columns != 'isZoonotic'], X_test_temp], axis=0, ignore_index=True)\n",
    "\n",
    "# # print(X_vals)\n",
    "y_vals = pd.concat([orig_df['isZoonotic'], y_test_temp], axis=0, ignore_index=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vals, y_vals, test_size=0.2, random_state=1)\n",
    "\n",
    "# print(len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65,0.7, 0.75, 0.8, 0.85, 1]\n",
    "\n",
    "parameters={\n",
    "   \"n_estimators\":130, # 200 kind of overfits I think\n",
    "    \"max_features\":2,\n",
    "    \"max_depth\":9,\n",
    "    \"random_state\":0,\n",
    "    \"min_sample_split\":25,\n",
    "    \"subsample\":0.85\n",
    "    # \"warm_start\":True\n",
    "}\n",
    "\n",
    "\n",
    "# careful with WARM START - only works after a lot of iterations\n",
    "# integrating both datasets works better!\n",
    "\n",
    "print(len(X_train)+len(X_test))\n",
    "for learning_rate in lr_list:\n",
    "    gradBoost = GradientBoostingClassifier(n_estimators=parameters['n_estimators'], \n",
    "    learning_rate=learning_rate, max_features=parameters['max_features'], \n",
    "    max_depth=parameters['max_depth'], random_state=parameters['random_state'], \n",
    "    min_samples_split=parameters['min_sample_split'], subsample=parameters['subsample']\n",
    "    )\n",
    "\n",
    "    parameters['learning_rate']=learning_rate\n",
    "    gradBoost.fit(X_train, y_train)\n",
    "\n",
    "    cols_when_model_builds = gradBoost.feature_names_in_\n",
    "    X_test=X_test[cols_when_model_builds]\n",
    "    \n",
    "    testingAcc = accuracy_score(y_test, gradBoost.predict(X_test))\n",
    "    trainingAcc = accuracy_score(y_train, gradBoost.predict(X_train))\n",
    "    \n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    # print(\"Accuracy score (training): {0:.3f}\".format(trainingAcc))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(testingAcc))\n",
    "    # print(f\"Feature importance {gradBoost.feature_importances_}\")\n",
    "\n",
    "    # pickle.dump(gradBoost, open('gradBoost.pkl', 'wb'))\n",
    "    saveModel(gradBoost, \"gradBoost\", X_test, y_test, parameters, gradBoost=True, dir=\"models/nardus_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "parameters={\n",
    "   \"n_estimators\":100, # 200 kind of overfits I think\n",
    "    \"max_features\":2,\n",
    "    \"max_depth\":9,\n",
    "    \"random_state\":0,\n",
    "    \"min_sample_split\":25,\n",
    "    \"subsample\":0.85\n",
    "    # \"warm_start\":True\n",
    "}\n",
    "\n",
    "\n",
    "param_test1 = {'n_estimators':range(100,150,10), 'learning_rate':[0.1,0.15, 0.2,0.25,0.3], 'subsample':[0.8,0.85,0.9], 'max_depth':range(6,10,1), 'min_samples_split':range(10,40,10)}\n",
    "\n",
    "gradBoost = GridSearchCV(estimator = GradientBoostingClassifier(\n",
    "    n_estimators=parameters['n_estimators'], \n",
    "learning_rate=learning_rate, max_features=parameters['max_features'], \n",
    "max_depth=parameters['max_depth'], random_state=parameters['random_state'], \n",
    "min_samples_split=parameters['min_sample_split'], subsample=parameters['subsample']\n",
    "), \n",
    "param_grid = param_test1, scoring='roc_auc',n_jobs=-1, cv=5, verbose=10)\n",
    "\n",
    "parameters['learning_rate']=learning_rate\n",
    "gradBoost.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(gradBoost, open('models/nardus_gridsearch.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = pickle.load(open('models/nardus_gridsearch.pkl', 'rb')).best_estimator_\n",
    "print(len(X_test_temp))\n",
    "cols_when_model_builds = gradBoost.feature_names_in_\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "\n",
    "print(accuracy_score(y_test_temp, gradBoost.predict(X_test_temp)))\n",
    "print(confusion_matrix(y_test_temp, gradBoost.predict(X_test_temp)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = pickle.load(open('models/nardus_gridsearch.pkl', 'rb')).best_estimator_\n",
    "\n",
    "cols_when_model_builds = gradBoost.feature_names_in_\n",
    "X_test=X_test[cols_when_model_builds]\n",
    "\n",
    "print(accuracy_score(y_test, gradBoost.predict(X_test)))\n",
    "print(confusion_matrix(y_test, gradBoost.predict(X_test)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = pickle.load(open('models/curr_models/gradBoost.pkl', 'rb'))\n",
    "print(len(X_test_temp))\n",
    "cols_when_model_builds = gradBoost.feature_names_in_\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "\n",
    "print(accuracy_score(y_test_temp, gradBoost.predict(X_test_temp)))\n",
    "print(confusion_matrix(y_test_temp, gradBoost.predict(X_test_temp)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = pickle.load(open('models/nardus_gridsearch.pkl', 'rb')).best_estimator_\n",
    "cols_when_model_builds = gradBoost.feature_names_in_\n",
    "X_test_temp=X_test_temp[cols_when_model_builds]\n",
    "\n",
    "print(accuracy_score(y_test_temp, gradBoost.predict(X_test_temp)).ravel())\n",
    "\n",
    "print(confusion_matrix(y_test_temp, gradBoost.predict(X_test_temp)).ravel())\n",
    "\n",
    "# ALWAYS reset X columns to the right order\n",
    "y_thing = y_test_temp.to_numpy()\n",
    "precision, recall, thresholds = precision_recall_curve(y_thing, gradBoost.predict_proba(X_test_temp)[::,1])\n",
    "aaa = auc(recall, precision)\n",
    "\n",
    "print(\"precision recall: \" + str(aaa))\n",
    "\n",
    "y_pred_proba = gradBoost.predict_proba(X_test_temp)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_test_temp, y_pred_proba)\n",
    "auc_thing = roc_auc_score(y_test_temp, y_pred_proba)\n",
    "\n",
    "print(y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc_thing))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradBoost = pickle.load(open('models/nardus_gridsearch.pkl', 'rb')).best_estimator_\n",
    "cols_when_model_builds = gradBoost.feature_names_in_\n",
    "X_test=X_test[cols_when_model_builds]\n",
    "\n",
    "print(accuracy_score(y_test, gradBoost.predict(X_test)).ravel())\n",
    "\n",
    "print(confusion_matrix(y_test, gradBoost.predict(X_test)).ravel())\n",
    "\n",
    "# ALWAYS reset X columns to the right order\n",
    "y_thing = y_test.to_numpy()\n",
    "precision, recall, thresholds = precision_recall_curve(y_thing, gradBoost.predict_proba(X_test)[::,1])\n",
    "aaa = auc(recall, precision)\n",
    "\n",
    "print(\"precision recall: \" + str(aaa))\n",
    "\n",
    "y_pred_proba = gradBoost.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_thing = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc_thing))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65,0.7, 0.75, 0.8, 0.85, 1]\n",
    "\n",
    "parameters={\n",
    "   \"n_estimators\":100, # 200 kind of overfits I think\n",
    "    \"max_features\":2,\n",
    "    \"max_depth\":8,\n",
    "    \"random_state\":0,\n",
    "    \"subsample\":0.8,\n",
    "    'lambda': 0.5, # regularization?\n",
    "    'alpha': 0.5\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    xgBoost = XGBClassifier(n_estimators=parameters['n_estimators'], \n",
    "    learning_rate=learning_rate, \n",
    "    max_depth=parameters['max_depth'], random_state=parameters['random_state'], \n",
    "    subsample=parameters['subsample'], \n",
    "    )\n",
    "\n",
    "    parameters['learning_rate']=learning_rate\n",
    "    xgBoost.fit(X_train, y_train)\n",
    "\n",
    "    # ALWAYS reset feature names\n",
    "    cols_when_model_builds = xgBoost.get_booster().feature_names\n",
    "    X_test=X_test[cols_when_model_builds]\n",
    "\n",
    "    testingAcc = accuracy_score(y_test, xgBoost.predict(X_test))\n",
    "    trainingAcc = accuracy_score(y_train, xgBoost.predict(X_train))\n",
    "    \n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    # print(\"Accuracy score (training): {0:.3f}\".format(trainingAcc))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(testingAcc))\n",
    "    # print(f\"Feature importance {gradBoost.feature_importances_}\")\n",
    "\n",
    "    # pickle.dump(gradBoost, open('gradBoost.pkl', 'wb'))\n",
    "    # saveModel(xgBoost, \"xgBoost\", X_test, y_test, parameters, xgBoost=True, dir=\"synthetic_data_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest = pickle.load(open('curr_models/randforest.pkl', 'rb'))\n",
    "print(accuracy_score(y_train, randforest.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = pickle.load(open('curr_models/knn.pkl', 'rb'))\n",
    "\n",
    "print(accuracy_score(y_test, knn.predict(X_test)))\n",
    "# knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSingleSequence(accessionID):\n",
    "    try:\n",
    "        QueryHandle = Entrez.efetch(db=\"nucleotide\", id=accessionID, \n",
    "                                    rettype=\"gb\", retmode=\"text\")\n",
    "    except HTTPError as Error:\n",
    "        if Error.code == 400:  # Bad request\n",
    "            raise ValueError(f\"Accession number not found: {accessionID}\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    SeqRec = SeqIO.read(QueryHandle, \"genbank\")\n",
    "    print(str(SeqRec))\n",
    "\n",
    "    oDict = assign_kmers_to_dict(X_info, permset, kmer) # convert ordereddict to pandas dataframe\n",
    "\n",
    "    kmer_df = pd.DataFrame()\n",
    "\n",
    "    for i in oDict:\n",
    "        kmer_df.at[0, i]=oDict[i]\n",
    "    # print(best_gradBoost.predict_proba(kmer_df))\n",
    "\n",
    "    cols_when_model_builds = best_gradBoost.feature_names_in_\n",
    "    kmer_df=kmer_df[cols_when_model_builds]\n",
    "    \n",
    "    print([round(x, 2) for x in best_gradBoost.predict_proba(kmer_df).tolist()[0]])\n",
    "    print(best_gradBoost.predict(kmer_df).tolist()[0])\n",
    "\n",
    "    # ls = []\n",
    "    # ls.append({'accession': accessionID, 'sequence': str(SeqRec.seq).lower()})\n",
    "    # df = pd.DataFrame(ls)\n",
    "    # print(df)\n",
    "\n",
    "getSingleSequence(\"PA544053\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('seq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct 19 2022, 17:52:09) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1360c63304de9435a2a3572d38e6c9496b6fb5d1617f35fbc8638664d664ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
