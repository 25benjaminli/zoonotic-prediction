{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/benjaminli/Documents/coding/scires/project/utils\n",
      "/Users/benjaminli/Documents/coding/scires/project/utils\n",
      "/Users/benjaminli/Documents/coding/scires/project\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations, product\n",
    "\n",
    "import tqdm\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from warnings import simplefilter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pickle\n",
    "import threading\n",
    "\n",
    "if (os.path.abspath('').split('/')[-1] == 'project'):\n",
    "    %cd utils\n",
    "elif (os.path.abspath('').split('/')[-1] == 'train_and_vis'):\n",
    "    %cd ../utils\n",
    "\n",
    "\n",
    "import query_utils\n",
    "import model_utils\n",
    "import validation_utils\n",
    "import data_utils\n",
    "from StackingClassifier import StackingClassifier\n",
    "\n",
    "print(os.path.abspath(''))\n",
    "\n",
    "if (os.path.abspath('').split('/')[-1] == 'utils'):\n",
    "    %cd ..\n",
    "\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working directory: /Users/benjaminli/Documents/coding/scires/project\n"
     ]
    }
   ],
   "source": [
    "dataset = data_utils.retrieveMerged(dir='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576\n"
     ]
    }
   ],
   "source": [
    "x = np.concatenate([dataset['merged']['normalized-4']['y_train'], dataset['merged']['normalized-4']['y_test']], axis=0).sum()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model, threshold, dataset):\n",
    "    ds = dataset['f1-4']\n",
    "    # mergedX = merged['X_test']\n",
    "    # mergedY = merged['y_test']\n",
    "    X, y = ds['X'], ds['y']\n",
    "\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    bp = {\n",
    "        'ans': {\n",
    "            '0': {'1000': 0, '3000': 0, '5000': 0, '10000': 0, '>10000': 0},\n",
    "            '1': {'1000': 0, '3000': 0, '5000': 0, '10000': 0, '>10000': 0},\n",
    "        },\n",
    "        'preds': {\n",
    "            '0': {'1000': 0, '3000': 0, '5000': 0, '10000': 0, '>10000': 0},\n",
    "            '1': {'1000': 0, '3000': 0, '5000': 0, '10000': 0, '>10000': 0},\n",
    "        }\n",
    "    }\n",
    "    ds = dataset['f2-4']\n",
    "\n",
    "    # mergedX = merged['X_test']\n",
    "    # mergedY = merged['y_test']\n",
    "    X, y = ds['X'], ds['y']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    threads = []\n",
    "\n",
    "    if (type(model).__name__ == 'XGBClassifier'):\n",
    "        X_test = X_test[model.get_booster().feature_names]\n",
    "\n",
    "    for r in tqdm.tqdm(range(len(X_test))):\n",
    "        data = pd.DataFrame([X_test.iloc[r].to_list()], columns=X_test.iloc[r].index)\n",
    "        # print(data)\n",
    "        pred_proba = model.predict_proba(data)[:,1]\n",
    "        pred = '1' if pred_proba >= threshold else '0'\n",
    "        s = X_test_reg.iloc[r].sum()\n",
    "        res = str(y_test_reg[r])\n",
    "        if s < 1000:\n",
    "            if pred == res:\n",
    "                bp['preds'][res]['1000']+=1\n",
    "            bp['ans'][res]['1000']+=1\n",
    "        elif s < 3000:\n",
    "            if pred == res:\n",
    "                bp['preds'][res]['3000']+=1\n",
    "            bp['ans'][res]['3000']+=1\n",
    "        elif s < 5000:\n",
    "            if pred == res:\n",
    "                bp['preds'][res]['5000']+=1\n",
    "            bp['ans'][res]['5000']+=1\n",
    "        elif s <= 10000:\n",
    "            if pred == res:\n",
    "                bp['preds'][res]['10000']+=1\n",
    "            bp['ans'][res]['10000']+=1\n",
    "        elif s > 10000:\n",
    "            if pred == res:\n",
    "                bp['preds'][res]['>10000']+=1\n",
    "            bp['ans'][res]['>10000']+=1\n",
    "\n",
    "    # for r in tqdm.tqdm(range(len(X_test))):\n",
    "    #     data = pd.DataFrame([X_test.iloc[r].to_list()], columns=mergedXNormed.iloc[r].index)\n",
    "    #     # print(data)\n",
    "    #     pred_proba = model.predict_proba(data)[:,1]\n",
    "    #     pred = 1 if pred_proba >= 0.5 else 0\n",
    "    print(bp['ans'])\n",
    "    print(bp['preds'])\n",
    "    # print accuracies for each bp length\n",
    "    zoo = 0\n",
    "    zoo_total = 0\n",
    "    nonzoo = 0\n",
    "    nonzoo_total = 0\n",
    "    for k in bp['ans']:\n",
    "        # get overall accuracies for varying values of k\n",
    "        for i in bp['ans'][k]:\n",
    "            if k == '1':\n",
    "                zoo += bp['preds'][k][i]\n",
    "                zoo_total += bp['ans'][k][i]\n",
    "            else:\n",
    "                nonzoo += bp['preds'][k][i]\n",
    "                nonzoo_total += bp['ans'][k][i]\n",
    "            print(k, i, bp['preds'][k][i]/bp['ans'][k][i])\n",
    "    \n",
    "    print(\"zoonotic score\", zoo/zoo_total)\n",
    "    print(\"nonzoonotic score\", nonzoo/nonzoo_total)\n",
    "   \n",
    "\n",
    "    # calculate f1 score\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2126/2126 [02:09<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'1000': 19, '3000': 275, '5000': 189, '10000': 447, '>10000': 890}, '1': {'1000': 7, '3000': 31, '5000': 32, '10000': 173, '>10000': 63}}\n",
      "{'0': {'1000': 19, '3000': 274, '5000': 185, '10000': 443, '>10000': 886}, '1': {'1000': 6, '3000': 23, '5000': 26, '10000': 167, '>10000': 60}}\n",
      "0 1000 1.0\n",
      "0 3000 0.9963636363636363\n",
      "0 5000 0.9788359788359788\n",
      "0 10000 0.9910514541387024\n",
      "0 >10000 0.9955056179775281\n",
      "1 1000 0.8571428571428571\n",
      "1 3000 0.7419354838709677\n",
      "1 5000 0.8125\n",
      "1 10000 0.9653179190751445\n",
      "1 >10000 0.9523809523809523\n",
      "zoonotic score 0.9215686274509803\n",
      "nonzoonotic score 0.9928571428571429\n",
      "0.8342857142857144\n"
     ]
    }
   ],
   "source": [
    "ds = dataset['f2-4']\n",
    "\n",
    "X,y = ds['X'], ds['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = pickle.load(open('models/curr_models/final_merged_stackingcv.pkl', 'rb'))\n",
    "knn = pickle.load(open('models/curr_models/knn.pkl', 'rb'))\n",
    "# rf = pickle.load(open('models/curr_models/randforest.pkl', 'rb'))\n",
    "mlp = pickle.load(open('models/curr_models/mlpClassifier.pkl', 'rb')).best_estimator_\n",
    "# xgboost = pickle.load(open('models/curr_models/xgBoost-f2-4-2.pkl', 'rb'))\n",
    "svm = pickle.load(open('models/curr_models/svm.pkl', 'rb'))\n",
    "em = pickle.load(open('models/curr_models/em-f2-4.pkl', 'rb'))\n",
    "\n",
    "# print(f1_score(y_test_reg, em.predict(X_test_reg)))\n",
    "# print(f1_score(y_test_reg, model.predict(X_test_reg)))\n",
    "# print(f1_score(y_test_reg, knn.predict(X_test_reg)))\n",
    "# print(f1_score(y_test_reg, rf.predict(X_test_reg)))\n",
    "# print(f1_score(y_test_reg, mlp.predict(X_test_reg)))\n",
    "# # print(f1_score(y_test_reg, xgboost.predict(X_test_reg)))\n",
    "# print(f1_score(y_test_reg, svm.predict(X_test_reg)))\n",
    "# print(f1_score(y_test_reg, xgbgrid.predict(X_test_reg)))\n",
    "\n",
    "\n",
    "# assess_model(model=em, threshold=0.3, dataset=dataset)\n",
    "# assess_model(model=model, threshold=0.5, dataset=dataset)\n",
    "\n",
    "# make into threads\n",
    "assess_model(em, 0.5, dataset)\n",
    "\n",
    "# assess_model(model=em2, threshold=0.5, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zoonotic</th>\n",
       "      <th>pred_zoonotic</th>\n",
       "      <th>overall</th>\n",
       "      <th>pred_overall</th>\n",
       "      <th>pred_overall%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>335</td>\n",
       "      <td>325</td>\n",
       "      <td>0.970149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>217</td>\n",
       "      <td>211</td>\n",
       "      <td>0.972350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>182</td>\n",
       "      <td>175</td>\n",
       "      <td>642</td>\n",
       "      <td>633</td>\n",
       "      <td>0.985981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;10000</th>\n",
       "      <td>66</td>\n",
       "      <td>60</td>\n",
       "      <td>923</td>\n",
       "      <td>913</td>\n",
       "      <td>0.989166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        zoonotic  pred_zoonotic  overall  pred_overall  pred_overall%\n",
       "1000           8              6       19            17       0.894737\n",
       "3000          45             37      335           325       0.970149\n",
       "5000          28             23      217           211       0.972350\n",
       "10000        182            175      642           633       0.985981\n",
       ">10000        66             60      923           913       0.989166"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(19/(19+335+217+642+923))\n",
    "df = pd.DataFrame()\n",
    "# fill df with the bp lengths as the index and the number of zoonotic and nonzoonotic as the columns\n",
    "df['zoonotic'] = pd.Series(di_zoo)\n",
    "df['pred_zoonotic'] = pd.Series(pred_di_zoo)\n",
    "df['overall'] = pd.Series(di_overall)\n",
    "df['pred_overall'] = pd.Series(pred_di_overall)\n",
    "df['pred_overall%'] = df['pred_overall']/df['overall']\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1360c63304de9435a2a3572d38e6c9496b6fb5d1617f35fbc8638664d664ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
