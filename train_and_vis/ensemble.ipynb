{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/benjaminli/Documents/coding/scires/project/utils\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations, product\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, balanced_accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean,std\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ctgan import CTGANSynthesizer\n",
    "\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from warnings import simplefilter\n",
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "z = False\n",
    "try:\n",
    "    %cd ../utils\n",
    "except:\n",
    "    z=True\n",
    "import model_utils\n",
    "import validation_utils\n",
    "if z:\n",
    "    %cd ..\n",
    "\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/benjaminli/Documents/coding/scires/project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/benjaminli/Documents/coding/scires/project'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/info.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'isZoonotic'], df['isZoonotic'], test_size=0.6, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/tomquisel/a421235422fdf6b51ec2ccc5e3dee1b4\n",
    "class VotingClassifier(object):\n",
    "    \"\"\"Stripped-down version of VotingClassifier that uses prefit estimators\"\"\"\n",
    "    def __init__(self, estimators, voting='hard', weights=None):\n",
    "        self.estimators = [e[1] for e in estimators]\n",
    "        self.named_estimators = dict(estimators)\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        ----------\n",
    "        maj : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # check_is_fitted(self, 'estimators')\n",
    "        if self.voting == 'soft':\n",
    "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "        else:  # 'hard' voting\n",
    "            predictions = self._predict(X)\n",
    "            maj = np.apply_along_axis(lambda x:\n",
    "                                      np.argmax(np.bincount(x,\n",
    "                                                weights=self.weights)),\n",
    "                                      axis=1,\n",
    "                                      arr=predictions.astype('int'))\n",
    "        return maj\n",
    "\n",
    "    def _collect_probas(self, X):\n",
    "        \"\"\"Collect results from clf.predict calls. \"\"\"\n",
    "        return np.asarray([clf.predict_proba(X) for clf in self.estimators])\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X in 'soft' voting \"\"\"\n",
    "        if self.voting == 'hard':\n",
    "            raise AttributeError(\"predict_proba is not available when\"\n",
    "                                 \" voting=%r\" % self.voting)\n",
    "        # check_is_fitted(self, 'estimators')\n",
    "        avg = np.average(self._collect_probas(X), axis=0, weights=self.weights)\n",
    "        return avg\n",
    "\n",
    "    @property\n",
    "    def predict_proba(self):\n",
    "        \"\"\"Compute probabilities of possible outcomes for samples in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        ----------\n",
    "        avg : array-like, shape = [n_samples, n_classes]\n",
    "            Weighted average probability for each class per sample.\n",
    "        \"\"\"\n",
    "        return self._predict_proba\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Return class labels or probabilities for X for each estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        If `voting='soft'`:\n",
    "          array-like = [n_classifiers, n_samples, n_classes]\n",
    "            Class probabilities calculated by each classifier.\n",
    "        If `voting='hard'`:\n",
    "          array-like = [n_samples, n_classifiers]\n",
    "            Class labels predicted by each classifier.\n",
    "        \"\"\"\n",
    "        # check_is_fitted(self, 'estimators')\n",
    "        if self.voting == 'soft':\n",
    "            return self._collect_probas(X)\n",
    "        else:\n",
    "            return self._predict(X)\n",
    "\n",
    "    def _predict(self, X):\n",
    "        \"\"\"Collect results from clf.predict calls. \"\"\"\n",
    "        return np.asarray([clf.predict(X) for clf in self.estimators]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9849770236832803\n",
      "0.8506539413220219\n",
      "0.9211735595616826\n",
      "0.9814422057264051\n",
      "0.9906327324142806\n"
     ]
    }
   ],
   "source": [
    "origGradBoost = pickle.load(open('models/curr_models/gradBoost.pkl', 'rb'))\n",
    "origXGBoost = pickle.load(open('models/curr_models/xgBoost.pkl', 'rb'))\n",
    "origLogReg = pickle.load(open('models/curr_models/lrmodel.pkl', 'rb'))\n",
    "origKNN = pickle.load(open('models/curr_models/knn.pkl', 'rb'))\n",
    "origRandForest = pickle.load(open('models/curr_models/randforest.pkl', 'rb'))\n",
    "origMLP = pickle.load(open('models/curr_models/mlpClassifier.pkl', 'rb'))\n",
    "\n",
    "nardusGradBoost = pickle.load(open('models/nardus_gridsearch.pkl', 'rb'))\n",
    "\n",
    "estimators=[('gradBoost', origGradBoost), ('nardusgradBoost', nardusGradBoost), ('rf', origRandForest), ('mlp', origMLP)]\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "print(accuracy_score(y_test, ensemble.predict(X_test)))\n",
    "\n",
    "print(accuracy_score(y_test, origGradBoost.predict(X_test)))\n",
    "print(accuracy_score(y_test, origKNN.predict(X_test)))\n",
    "print(accuracy_score(y_test, origMLP.predict(X_test)))\n",
    "print(accuracy_score(y_test, nardusGradBoost.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1360c63304de9435a2a3572d38e6c9496b6fb5d1617f35fbc8638664d664ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
