{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations, product\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, balanced_accuracy_score, precision_recall_curve, auc, roc_curve, roc_auc_score, f1_score, recall_score, precision_score, brier_score_loss, average_precision_score, classification_report, log_loss\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from collections import Counter\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean,std\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ctgan import CTGANSynthesizer\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "from os import path\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from warnings import simplefilter\n",
    "from collections import OrderedDict\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "if (os.path.abspath('').split('/')[-1] == 'project'):\n",
    "    %cd utils\n",
    "elif (os.path.abspath('').split('/')[-1] == 'train_and_vis'):\n",
    "    %cd ../utils\n",
    "\n",
    "import query_utils\n",
    "import model_utils\n",
    "import validation_utils\n",
    "import data_utils\n",
    "\n",
    "if (os.path.abspath('').split('/')[-1] == 'utils'):\n",
    "    %cd ..\n",
    "\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data creation\n",
    "Performed with CTGANSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isZoonotic = df.loc[df['isZoonotic']==1][:1200]\n",
    "isZoonotic = isZoonotic.loc[:, isZoonotic.columns != 'isZoonotic']\n",
    "print(isZoonotic)\n",
    "\n",
    "posGanModel = CTGANSynthesizer(batch_size=60, epochs=10, verbose=True)\n",
    "posGanModel.fit(isZoonotic)\n",
    "\n",
    "# check if current model is better than pickled model\n",
    "posGanModel.save('models/curr_models/posGanModel.pkl')\n",
    "\n",
    "notZoonotic = df.loc[df['isZoonotic']==0][:3000]\n",
    "notZoonotic = isZoonotic.loc[:, isZoonotic.columns != 'isZoonotic']\n",
    "print(notZoonotic)\n",
    "\n",
    "negGanModel = CTGANSynthesizer(batch_size=60, epochs=10, verbose=True)\n",
    "negGanModel.fit(notZoonotic)\n",
    "negGanModel.save('models/curr_models/negGanModel.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset retrieval\n",
    "Workings of the function is packaged into data_utils (for readability). Data is generated within \"process_data.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_utils.retrieveMerged(dir='data/')\n",
    "# datasets = data_utils.retrieveAllDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['f1-3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset['f2-4']['X']))\n",
    "print(len(dataset['f2-4']['y'])-sum(dataset['f2-4']['y']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep track of scores of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelScores = {}\n",
    "import json\n",
    "modelScores = pickle.load(open('score_df.pkl', 'rb'))\n",
    "modelScores = modelScores.T.to_dict()\n",
    "modelScores\n",
    "\n",
    "score_df = pickle.load(open('score_df.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate & validate performance of KNN (baseline) on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelScores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 4\n",
    "\n",
    "features = ['f1', 'f2', 'f3']\n",
    "for kmer in range(3, 7):\n",
    "    for feature in features:\n",
    "        ds = dataset[f'{feature}-{kmer}']\n",
    "\n",
    "        X, y = ds['X'], ds['y']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        knntest = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors = 1, n_jobs = -1), n_estimators = 1, n_jobs = -1)\n",
    "\n",
    "        # knntest = KNeighborsClassifier(n_neighbors = 1, n_jobs = -1)\n",
    "        knntest.fit(X_train, y_train)\n",
    "        # print(knntest.score(X_test, y_test))\n",
    "        x = cross_validate(knntest, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision', ''])\n",
    "        \n",
    "        name = f'knn_{feature}_{kmer}'\n",
    "        \n",
    "        if (name not in modelScores):\n",
    "            modelScores[name] = {}\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "        else:\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "            print('already in modelScores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(modelScores).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & validate performance of random forest (baseline) on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 4\n",
    "\n",
    "features = ['f1', 'f2', 'f3']\n",
    "for kmer in range(3, 7):\n",
    "    for feature in features:\n",
    "        ds = dataset[f'{feature}-{kmer}']\n",
    "\n",
    "        X, y = ds['X'], ds['y']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # randforest = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(max_features=\"sqrt\"), n_estimators=1, n_jobs=-1)\n",
    "        randforest = BalancedRandomForestClassifier(max_features=\"sqrt\", n_jobs=-1)\n",
    "\n",
    "        # randforest.fit(X_train, y_train)\n",
    "        randforest.fit(X_train, y_train)\n",
    "        x = cross_validate(randforest, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision'])\n",
    "        name = f'rf_{feature}_{kmer}'\n",
    "        if (name not in modelScores):\n",
    "            modelScores[name] = {}\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "        else:\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "            print('already in modelScores')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 4\n",
    "\n",
    "features = ['f1', 'f2', 'f3']\n",
    "for kmer in range(3, 7):\n",
    "    for feature in features:\n",
    "        ds = dataset[f'{feature}-{kmer}']\n",
    "\n",
    "        X, y = ds['X'], ds['y']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # randforest = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(max_features=\"sqrt\"), n_estimators=1, n_jobs=-1)\n",
    "        xgb1 = BalancedBaggingClassifier(base_estimator=XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=300,\n",
    "        max_depth=9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        #  scale_pos_weight=1,\n",
    "        seed=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=6,\n",
    "        ), n_estimators=1, n_jobs=-1)\n",
    "        xgb1.fit(X_train, y_train)\n",
    "\n",
    "        # randforest.fit(X_train, y_train)\n",
    "        x = cross_validate(xgb1, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision'])\n",
    "        name = f'xgb_{feature}_{kmer}'\n",
    "        if (name not in modelScores):\n",
    "            modelScores[name] = {}\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "        else:\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "            print('already in modelScores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelScores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = datasets['merged']['lengthdivdataset-4']\n",
    "features = ['f1', 'f2', 'f3']\n",
    "for kmer in range(3, 7):\n",
    "    for feature in features:\n",
    "        ds = dataset[f'{feature}-{kmer}']\n",
    "\n",
    "        X, y = ds['X'], ds['y']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # randforest = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(max_features=\"sqrt\"), n_estimators=1, n_jobs=-1)\n",
    "        mlp = BalancedBaggingClassifier(base_estimator=MLPClassifier(alpha=0.6, hidden_layer_sizes=(100, 180, 180, 200, 200),\n",
    "              max_iter=550, random_state=42, solver='adam', activation='relu'), n_estimators=5, n_jobs=-1)\n",
    "        mlp.fit(X_train, y_train)\n",
    "\n",
    "        # randforest.fit(X_train, y_train)\n",
    "        x = cross_validate(mlp, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision']])\n",
    "        name = f'mlp_{feature}_{kmer}'\n",
    "        if (name not in modelScores):\n",
    "            modelScores[name] = {}\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "        else:\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "            print('already in modelScores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_time 71.89677076339721\n",
    "score_time 0.10850300788879394\n",
    "test_recall 0.8584950773558369\n",
    "test_f1 0.6911281484361098\n",
    "test_accuracy 0.8864734363076601\n",
    "test_precision 0.5796959595204415\n",
    "test_roc_auc 0.9540192015137766\n",
    "test_neg_brier_score -0.07964541647825815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(modelScores.keys()))\n",
    "# print(modelScores.keys())\n",
    "# print(modelScores['mlp_balanced_normalized_4'])\n",
    "# print(modelScores['mlp_balanced_normalized_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['f1', 'f2', 'f3']\n",
    "for kmer in range(3, 7):\n",
    "    for feature in features:\n",
    "        ds = dataset[f'{feature}-{kmer}']\n",
    "\n",
    "        X, y = ds['X'], ds['y']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # randforest = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(max_features=\"sqrt\"), n_estimators=1, n_jobs=-1)\n",
    "        temp_svm = BalancedBaggingClassifier(base_estimator=SVC(kernel='rbf', C=2, gamma=0.6, probability=True, random_state=42, max_iter=500), n_estimators=1, n_jobs=-1)\n",
    "        temp_svm.fit(X_train, y_train)\n",
    "\n",
    "        # randforest.fit(X_train, y_train)\n",
    "        x = cross_validate(temp_svm, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score'])\n",
    "        name = f'svm_{feature}_{kmer}'\n",
    "        if (name not in modelScores):\n",
    "            modelScores[name] = {}\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "        else:\n",
    "            for k, v in x.items():\n",
    "                print(k, v.mean())\n",
    "                modelScores[name][k]=v.mean()\n",
    "            print('already in modelScores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "xgb1_test = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=200,\n",
    "        max_depth=9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        #  scale_pos_weight=1,\n",
    "        seed=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=6,\n",
    ")\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "xgb1_test.fit(X_train, y_train, eval_metric='aucpr', eval_set=[(X_validation, y_validation)], early_stopping_rounds=10, verbose=10)\n",
    "# xgb1.fit(X_train, y_train)\n",
    "\n",
    "# x = cross_validate(xgb1, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score'])\n",
    "\n",
    "# for k, v in x.items():\n",
    "#         print(k, v.mean())\n",
    "        # modelScores[name][k]=v.mean()\n",
    "\n",
    "# X_test = X_test[xgb1.get_booster().feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cross_validate(xgb1, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision'])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train random forest\n",
    "randforest = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(max_features=\"sqrt\"), n_estimators=1, n_jobs=-1)\n",
    "randforest.fit(X_train, y_train)\n",
    "\n",
    "# train svm\n",
    "temp_svm = BalancedBaggingClassifier(base_estimator=SVC(kernel='rbf', C=2, gamma=0.6, probability=True, random_state=42), n_estimators=1, n_jobs=-1)\n",
    "temp_svm.fit(X_train, y_train)\n",
    "\n",
    "# train knn\n",
    "knn = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=1, n_jobs=-1), n_estimators=1, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb1_test, open('models/curr_models/xgBoost-f2-4-2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess each model\n",
    "xgb1 = pickle.load(open('models/curr_models/xgBoost-f2-4-2.pkl', 'rb'))\n",
    "\n",
    "models = [randforest, temp_svm, knn, xgb1]\n",
    "modelNames = ['randforest', 'svm', 'knn', 'xgboost']\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    modelName = modelNames[i]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    print(f'{modelName} recall: \\n{recall_score(y_test, y_pred)}')\n",
    "    print(f'{modelName} f1: \\n{f1_score(y_test, y_pred)}')\n",
    "    print(f'{modelName} accuracy: \\n{accuracy_score(y_test, y_pred)}')\n",
    "    print(f'{modelName} precision: \\n{precision_score(y_test, y_pred)}')\n",
    "    print(f'{modelName} roc_auc: \\n{roc_auc_score(y_test, y_pred_proba)}')\n",
    "    print(f'{modelName} brier_score: \\n{brier_score_loss(y_test, y_pred_proba)}')\n",
    "    print(f'{modelName} confusion matrix: \\n{confusion_matrix(y_test, y_pred).ravel()}')\n",
    "    print(f'{modelName} classification report: \\n{classification_report(y_test, y_pred)}')\n",
    "    print(f'{modelName} roc_curve: \\n{roc_curve(y_test, y_pred_proba)}')\n",
    "    print(f'{modelName} precision_recall_curve: \\n{precision_recall_curve(y_test, y_pred_proba)}')\n",
    "    print(f'{modelName} average_precision_score: \\n{average_precision_score(y_test, y_pred_proba)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "mlp = BalancedBaggingClassifier(base_estimator=MLPClassifier(alpha=0.6, hidden_layer_sizes=(100, 180, 180, 200, 200),\n",
    "              max_iter=550, random_state=42, solver='adam', activation='relu'), n_estimators=5, n_jobs=-1)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# randforest.fit(X_train, y_train)\n",
    "x = cross_validate(mlp, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "        # modelScores[name][k]=v.mean()\n",
    "pickle.dump(mlp, open('models/curr_models/mlp-f2-4.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = pickle.load(open('models/curr_models/xgBoost-f2-4-2.pkl', 'rb'))\n",
    "randforest = pickle.load(open('models/curr_models/randforest-test.pkl', 'rb'))\n",
    "# get brier score\n",
    "print(f1_score(y_test, xgb1.predict(X_test)))\n",
    "print(brier_score_loss(y_test, xgb1.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "# get roc auc score\n",
    "print(roc_auc_score(y_test, xgb1.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "print(recall_score(y_test, xgb1.predict(X_test)))\n",
    "\n",
    "# get precision recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, xgb1.predict_proba(X_test)[:, 1])\n",
    "# plot the precision-recall curves\n",
    "print(auc(recall, precision))\n",
    "\n",
    "print(average_precision_score(y_test, xgb1.predict_proba(X_test)[:, 1]))\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='XGBoost')\n",
    "\n",
    "\n",
    "# plot precision recall for knn\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, randforest.predict_proba(X_test)[:, 1])\n",
    "print(auc(recall, precision))\n",
    "plt.plot(recall, precision, marker='.', label='rand forest')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(xgb1).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(modelScores['knn']['test_average_precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset[f'f2-4']\n",
    "\n",
    "X, y = ds['X'], ds['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# randforest = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(max_features=\"sqrt\"), n_estimators=1, n_jobs=-1)\n",
    "\n",
    "\n",
    "x = cross_validate(xgb1, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision'])\n",
    "\n",
    "for k, v in x.items():\n",
    "    print(k, v.mean())\n",
    "# print(accuracy_score(knn.predict(X_test), y_test))\n",
    "# print(recall_score(knn.predict(X_test), y_test))\n",
    "\n",
    "\n",
    "# print(accuracy_score(xgb1.predict(X_test), y_test))\n",
    "# print(recall_score(xgb1.predict(X_test), y_test))\n",
    "# print(f1_score(xgb1.predict(X_test), y_test))\n",
    "# pickle.dump(xgb1, open('models/curr_models/xgBoost.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdi = {}\n",
    "ds = dataset[f'f3-4']\n",
    "\n",
    "X, y = ds['X'], ds['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors = 1, n_jobs = -1), n_estimators = 1, n_jobs = -1)\n",
    "\n",
    "# randforest.fit(X_train, y_train)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, knn.predict_proba(X_test)[:,1])\n",
    "area = auc(recall, precision)\n",
    "\n",
    "\n",
    "print('Area Under Curve: %.2f' % area)\n",
    "\n",
    "x = cross_validate(knn, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision'])\n",
    "\n",
    "for k, v in x.items():\n",
    "    print(k, v.mean())\n",
    "    testdi[k]=v.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order models by performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelScores = pickle.load(open('model_scores.pkl', 'rb'))\n",
    "score_df = pd.DataFrame(modelScores).T\n",
    "\n",
    "# select model with best overall scores, precision doesn't really matter, excluding accuracy just because\n",
    "# score_df['total'] = score_df.apply(lambda x: x[['test_f1', 'test_recall', 'test_roc_auc', 'test_neg_brier_score']].sum(), axis=1)\n",
    "score_df['name'] = score_df.apply(lambda x: x.name.split(\"_\")[0], axis=1)\n",
    "# score_df['feature'] = score_df.apply(lambda x: x.name.split(\"_\")[1], axis=1)\n",
    "score_df['kmer'] = score_df.apply(lambda x: x.name.split(\"_\")[2], axis=1)\n",
    "# sort based on total column\n",
    "score_df = score_df.sort_values(by='test_f1', ascending=False)\n",
    "# print(len(score_df))\n",
    "\n",
    "# for each k-mer value, create a plot with the AUC score of each model and each feature and put it into one graph\n",
    "\n",
    "\n",
    "# for kmer in modelKmers:\n",
    "#     for name in modelNames:\n",
    "#         for feature in modelFeatures:\n",
    "#             df = score_df[score_df.index.str.contains(f'{name}_{feature}_{kmer}')]\n",
    "#             df.plot.bar(y=['test_f1', 'test_recall', 'test_roc_auc', 'test_neg_brier_score', 'test_accuracy'], figsize=(20, 10))\n",
    "#             plt.title(f'{name} {feature} {kmer}')\n",
    "#     plt.show()\n",
    "\n",
    "# retrieve all model names\n",
    "modelNames = score_df['name'].unique()\n",
    "\n",
    "for kmer in range(3, 7):\n",
    "    # for feature in features:\n",
    "\n",
    "    for modelName in modelNames:\n",
    "        # retrieve models that match the current name and k-mer\n",
    "        df = score_df[score_df['name'] == modelName]\n",
    "        df = df[df['kmer'] == str(kmer)]\n",
    "        # rename all indices to the name of the model\n",
    "        print(df)\n",
    "        df.index = df.apply(lambda x: x.name.split(\"_\")[0], axis=1)\n",
    "        \n",
    "        # plot the auc for \n",
    "        df.plot.bar(y=['test_roc_auc', 'test_accuracy'], figsize=(20, 10), rot=0, )\n",
    "        plt.title(f'kmer = {kmer}, feature = {feature}')\n",
    "    plt.show()\n",
    "\n",
    "# xg_boost = score_df[score_df.index.str.contains(f'{name}_{feature}_{kmer}')]\n",
    "# # mlp = score_df[score_df.index.str.contains('mlp')]\n",
    "# # svm = score_df[score_df.index.str.contains('svm')]\n",
    "# xg_boost.plot.bar(y=['test_f1', 'test_recall', 'test_roc_auc', 'test_neg_brier_score', 'test_accuracy'], figsize=(20, 10))\n",
    "\n",
    "# xg_boost.plot.bar(y=['test_f1', 'test_recall', 'test_roc_auc', 'test_neg_brier_score', 'test_accuracy'], figsize=(20, 10))\n",
    "# pickle.dump(score_df, open('score_df.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'test_f1'\n",
    "\n",
    "# Create a figure and subplots for each feature\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "# Loop through each k-mer length\n",
    "for length in range(3, 7):\n",
    "    # Loop through each feature\n",
    "    for i, feature in enumerate(['knn_f1', 'knn_f2', 'knn_f3']):\n",
    "        # Extract the data for the current length and feature\n",
    "        data = [modelScores[model][metric] for model in modelScores]\n",
    "        # print(data)\n",
    "        # Plot the data on the corresponding subplot\n",
    "        axs[i].plot(length, 'o-')\n",
    "        axs[i].set_xlabel('k-mer length')\n",
    "        axs[i].set_ylabel(metric)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump models into pickle - TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Searched version of the Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets['merged'][f'normalized-{kmer}']\n",
    "\n",
    "X_train, y_train, X_test, y_test = ds['X_train'], ds['y_train'], ds['X_test'], ds['y_test']\n",
    "\n",
    "\"\"\"\n",
    "{'n_estimators': 120, 'max_features': 2, 'max_depth': 6, 'random_state': 0, 'min_sample_split': 50, 'subsample': 0.8, 'learning_rate': 0.3}\n",
    "\"\"\"\n",
    "\n",
    "parameters={\n",
    "   'n_estimators': 120, 'max_features': 2, 'max_depth': 6, 'random_state': 42, 'min_sample_split': 50, 'subsample': 0.8, 'learning_rate': 0.3\n",
    "}\n",
    "\n",
    "param_test1 = {'n_estimators':range(100,140,10), 'learning_rate':[0.1,0.15,0.2], 'subsample':[0.8,0.85,0.9], 'max_depth':range(6,9,1), 'min_samples_split':range(10,40,10), 'max_features':range(2, 5)}\n",
    "\n",
    "gradBoost = GridSearchCV(estimator = GradientBoostingClassifier(\n",
    "    n_estimators=parameters['n_estimators'], max_features=parameters['max_features'], random_state=parameters['random_state']), \n",
    "param_grid = param_test1, scoring='roc_auc',n_jobs=-1, cv=5, verbose=10)\n",
    "\n",
    "# parameters['learning_rate']=learning_rate\n",
    "gradBoost.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "def hidden_layers_generator(hidden_layers, max_neurons):\n",
    "  hd_sizes = []\n",
    "  comb = combinations_with_replacement(np.arange(100,max_neurons+10,20), hidden_layers)\n",
    "  hd_sizes.append(list(comb))\n",
    "  return hd_sizes\n",
    "\n",
    "\n",
    "# ds = datasets['merged'][f'normalized-{kmer}']\n",
    "\n",
    "# X_train, y_train, X_test, y_test = ds['X_train'], ds['y_train'], ds['X_test'], ds['y_test']\n",
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = np.concatenate([ds['y_train'], ds['y_test']], axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(l)\n",
    "hlg = hidden_layers_generator(hidden_layers=5, max_neurons=200)\n",
    "print(hlg)\n",
    "\n",
    "mlp_gs = MLPClassifier(max_iter=350, random_state=42, solver='adam')\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': hlg[0],\n",
    "    'activation': ['relu'],\n",
    "    'alpha': [0.05, 0.1, 0.2],\n",
    "}\n",
    "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5, verbose=10, scoring='recall')\n",
    "clf.fit(X_train, y_train) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the ensemble model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pickle.load(open('score_df.pkl', 'rb'))\n",
    "\n",
    "# score_df = score_df.sort_values(by=['test'], ascending=False)\n",
    "a = set([x.split(\"_\")[0] for x in score_df.index.to_list()])\n",
    "print(a)\n",
    "\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open('model_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained models\n",
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# MLP: f2_4\n",
    "mlp = BalancedBaggingClassifier(base_estimator=MLPClassifier(alpha=0.6, hidden_layer_sizes=(100, 180, 180, 200, 200),\n",
    "              max_iter=550, random_state=42, solver='adam', activation='relu'), n_estimators=5, n_jobs=-1)\n",
    "\n",
    "mlp.fit(X_train.values, y_train)\n",
    "\n",
    "# knn\n",
    "knn = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=1, n_jobs=-1), n_estimators=1, n_jobs=-1)\n",
    "knn.fit(X_train.values, y_train)\n",
    "\n",
    "# SVM: f2_4\n",
    "# temp_svm = BalancedBaggingClassifier(base_estimator=SVC(kernel='rbf', C=2, gamma=0.6, probability=True, random_state=42), n_estimators=10, n_jobs=-1)\n",
    "\n",
    "# temp_svm.fit(X_train, y_train)\n",
    "\n",
    "# RF: f2_4\n",
    "randforest = BalancedRandomForestClassifier(max_features=\"sqrt\", n_jobs=-1)\n",
    "\n",
    "randforest.fit(X_train.values, y_train)\n",
    "\n",
    "# XGBoost: f2_4\n",
    "xgb1 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=200,\n",
    "        max_depth=9,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        #  scale_pos_weight=1,\n",
    "        seed=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=6,\n",
    ")\n",
    "X_train_xg, X_validation, y_train_xg, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "xgb1.fit(X_train_xg.values, y_train_xg, eval_metric='aucpr', eval_set=[(X_validation.values, y_validation)], early_stopping_rounds=20, verbose=10)\n",
    "# print(\"cross validating stacking classifier\")\n",
    "# print(em.cross_validate(X, y, cv=5))\n",
    "# xgb1 = pickle.load(open('models/curr_models/xgb1-test.pkl', 'rb'))\n",
    "\n",
    "\n",
    "# xgb1.fit(X_train, y_train)\n",
    "\n",
    "# em = StackingCVClassifier(classifiers = [mlp, randforest, xgb1],\n",
    "#                             # shuffle = True,\n",
    "#                             use_probas = True,\n",
    "#                             cv = 5,\n",
    "#                             use_features_in_secondary=True,\n",
    "#                             meta_classifier = LogisticRegression(C = 1, random_state=42, solver='saga'), n_jobs=-1, random_state=42, verbose=1, store_train_meta_features=True)\n",
    "# x = cross_validate(em, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score'], verbose=1, n_jobs=-1)\n",
    "# name = 'ensemble_lengthdiv_4'\n",
    "# if (name not in modelScores):\n",
    "#     modelScores[name] = {}\n",
    "#     for k, v in x.items():\n",
    "#         print(k, v.mean())\n",
    "#         modelScores[name][k]=v.mean()\n",
    "# else:\n",
    "#     print('already in modelScores')\n",
    "# em.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_validate(mlp, X_train, y_train, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'neg_log_loss'], verbose=1, n_jobs=-1))\n",
    "print(cross_validate(randforest, X_train, y_train, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'neg_log_loss'], verbose=1, n_jobs=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingClassifier():\n",
    "    def __init__(self, classifiers, meta_classifier, n_folds=5, use_probas=True):\n",
    "        self.classifiers = classifiers # assume pretrained\n",
    "        self.meta_classifier = meta_classifier # logistic regression\n",
    "        self.n_folds = n_folds\n",
    "        self.X_train_new=None\n",
    "        self.X_test_new=None\n",
    "        self.y_train_new=None\n",
    "        self.use_probas = use_probas\n",
    "        self.feature_names_in = None\n",
    "\n",
    "    def fit_pretrained(self, X_train, y_train):\n",
    "        self.X_train_new = np.zeros((X_train.shape[0], len(self.classifiers)))\n",
    "        self.y_train_new = y_train\n",
    "        print(X_train.shape[0], len(y_train))\n",
    "        \n",
    "        for i, clf in enumerate(self.classifiers):\n",
    "            if self.use_probas:\n",
    "                self.X_train_new[:, i] = model.predict_proba(X_train)[:,1]\n",
    "            else:\n",
    "                self.X_train_new[:, i] = model.predict(X_train)\n",
    "\n",
    "        print(len(self.X_train_new))\n",
    "        \n",
    "        self.meta_classifier = self.meta_classifier.fit(self.X_train_new, self.y_train_new)\n",
    "\n",
    "    def fit_not_pretrained(self, X_train, y_train, cv = 10, verbose=False): # assume NOT pretrained\n",
    "        print(X_train.shape[0], len(y_train))\n",
    "        kfold = StratifiedKFold(n_splits=cv, random_state=42, shuffle=True)\n",
    "\n",
    "        out_of_fold_predictions = np.zeros((X_train.shape[0], len(self.classifiers)))\n",
    "        \n",
    "        # Iterate over the folds\n",
    "\n",
    "        for i, clf in enumerate(self.classifiers):\n",
    "            print(\"fitting classifier\", i)\n",
    "            it = 0\n",
    "            for train_index, holdout_index in kfold.split(X_train, y_train):\n",
    "                print(\"fitting fold\", it)\n",
    "                # instance = clone(clf)\n",
    "                # self.base_estimators_[i].append(instance)\n",
    "                if type(clf).__name__ == 'XGBClassifier':\n",
    "                    print(\"xgboost detected\")\n",
    "                    X_train_xg, X_val, y_train_xg, y_val = train_test_split(X_train[train_index], y_train[train_index], test_size=0.15, random_state=1)\n",
    "                    self.classifiers[i] = clf.fit(X_train_xg, y_train_xg, eval_metric='aucpr', eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=10)\n",
    "                    \n",
    "                    if self.use_probas:\n",
    "                        y_pred = clf.predict_proba(X_train[holdout_index])[:,1]\n",
    "                        out_of_fold_predictions[holdout_index, i] = y_pred # set indexhere to the prediction value\n",
    "                    else:\n",
    "                        y_pred = clf.predict(X_train[holdout_index])\n",
    "                        out_of_fold_predictions[holdout_index, i] = y_pred # set indexhere to the prediction value\n",
    "                else:\n",
    "                    self.classifiers[i]=clf.fit(X_train[train_index], y_train[train_index])\n",
    "                    if self.use_probas:\n",
    "                        y_pred = clf.predict_proba(X_train[holdout_index])[:,1]\n",
    "                        out_of_fold_predictions[holdout_index, i] = y_pred # set indexhere to the prediction value\n",
    "                    else:\n",
    "                        y_pred = clf.predict(X_train[holdout_index])\n",
    "                        out_of_fold_predictions[holdout_index, i] = y_pred # set indexhere to the prediction value\n",
    "                it += 1\n",
    "\n",
    "        \n",
    "        self.meta_classifier.fit(out_of_fold_predictions, y_train)\n",
    "        pickle.dump(out_of_fold_predictions, open(\"base_predictions.pkl\", \"wb\"))\n",
    "        pickle.dump(y_train, open(\"y_truth.pkl\", \"wb\"))\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # make \n",
    "        meta_features = np.column_stack([\n",
    "            clf.predict(X) for clf in self.classifiers\n",
    "        ])\n",
    "        return self.meta_classifier.predict(meta_features)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            clf.predict_proba(X)[:,1] for clf in self.classifiers\n",
    "        ])\n",
    "        return self.meta_classifier.predict_proba(meta_features)\n",
    "\n",
    "    def cross_validate(self, X, y, scoring=['precision', 'recall', 'f1', 'average_precision', 'reg_prec', 'log_loss', 'neg_brier_score', 'roc_auc', 'accuracy'], cv=5):\n",
    "        kfold = StratifiedKFold(n_splits=cv, random_state=42, shuffle=True)\n",
    "        scores = {s: [] for s in scoring}\n",
    "        metrics = {\n",
    "            'recall': recall_score,\n",
    "            'f1': f1_score,\n",
    "            'accuracy': accuracy_score,\n",
    "            'precision': precision_score,\n",
    "            'roc_auc': roc_auc_score,\n",
    "            'neg_brier_score': brier_score_loss,\n",
    "            'average_precision': average_precision_score,\n",
    "            'reg_prec': precision_recall_curve,\n",
    "            'log_loss': log_loss\n",
    "        }\n",
    "        for train_index, test_index in kfold.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "            self.fit_not_pretrained(X_train, y_train)\n",
    "            if self.use_probas:\n",
    "                # print(y_pred.sum())\n",
    "                for s in scoring:\n",
    "                    if s == 'reg_prec' or s == 'average_precision' or s == 'neg_brier_score' or s == 'log_loss' or s == 'roc_auc':\n",
    "                        y_pred = self.predict_proba(X_test)[:,1]\n",
    "                        if s == 'reg_prec':\n",
    "                            precision, recall, _ = metrics[s](y_test, y_pred)\n",
    "                            print(\"auc: \", auc(recall, precision))\n",
    "                            scores[s].append((recall, precision))\n",
    "                            # print('regprec', auc(recall, precision))\n",
    "                        else:\n",
    "                            scores[s].append(metrics[s](y_test, y_pred))\n",
    "                        # print('regprec', auc(recall, precision))\n",
    "                    else:\n",
    "                        y_pred = self.predict(X_test)\n",
    "                        scores[s].append(metrics[s](y_test, y_pred))\n",
    "            else:\n",
    "                # print(\"not use probas\")\n",
    "                y_pred = self.predict(X_test)\n",
    "\n",
    "                # print(y_pred.sum())\n",
    "\n",
    "                for s in scoring:\n",
    "                    met = metrics[s](y_test, y_pred)\n",
    "                    # print(s, met)\n",
    "                    scores[s].append(met)\n",
    "\n",
    "        return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegression(C = 1, random_state=42, solver='saga')\n",
    "# BalancedBaggingClassifier(base_estimator=SVC(kernel='rbf', C=2, gamma=0.6, probability=True, random_state=42), n_jobs=-1)\n",
    "em = StackingClassifier(classifiers = [mlp, randforest, xgb1], use_probas = True, meta_classifier = LogisticRegression(C = 1, random_state=42, solver='saga'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.fit_not_pretrained(X_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(recall_score(y_test, em.predict(X_test.values)))\n",
    "print(recall_score(y_test, xgb1.predict(X_test.values)))\n",
    "print(recall_score(y_test, randforest.predict(X_test.values)))\n",
    "print(recall_score(y_test, mlp.predict(X_test.values)))\n",
    "\n",
    "print(log_loss(y_test, em.predict_proba(X_test.values)[:,1]))\n",
    "print(log_loss(y_test, xgb1.predict_proba(X_test.values)[:,1]))\n",
    "print(log_loss(y_test, randforest.predict_proba(X_test.values)[:,1]))\n",
    "print(log_loss(y_test, mlp.predict_proba(X_test.values)[:,1]))\n",
    "\n",
    "print(average_precision_score(y_test, em.predict_proba(X_test.values)[:,1]))\n",
    "print(average_precision_score(y_test, xgb1.predict_proba(X_test.values)[:,1]))\n",
    "print(average_precision_score(y_test, randforest.predict_proba(X_test.values)[:,1]))\n",
    "print(average_precision_score(y_test, mlp.predict_proba(X_test.values)[:,1]))\n",
    "\n",
    "print(accuracy_score(y_test, em.predict(X_test.values)))\n",
    "print(accuracy_score(y_test, xgb1.predict(X_test.values)))\n",
    "print(accuracy_score(y_test, randforest.predict(X_test.values)))\n",
    "print(accuracy_score(y_test, mlp.predict(X_test.values)))\n",
    "\n",
    "print(\"precision\")\n",
    "print(precision_score(y_test, em.predict(X_test.values)))\n",
    "print(precision_score(y_test, xgb1.predict(X_test.values)))\n",
    "print(precision_score(y_test, randforest.predict(X_test.values)))\n",
    "print(precision_score(y_test, mlp.predict(X_test.values)))\n",
    "\n",
    "ds = dataset['f3-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "knn = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=1, n_jobs=-1), n_estimators=1, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(average_precision_score(y_test, knn.predict_proba(X_test.values)[:,1]))\n",
    "print(accuracy_score(y_test, knn.predict(X_test.values)))\n",
    "print(recall_score(y_test, knn.predict(X_test.values)))\n",
    "print(log_loss(y_test, knn.predict_proba(X_test.values)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(mlp, open('models/curr_models/mlp-f2-4.pkl', 'wb'))\n",
    "# pickle.dump(em, open('models/curr_models/em-f2-4-test.pkl', 'wb'))\n",
    "# pickle.dump(randforest, open('models/curr_models/randforest-f2-4.pkl', 'wb'))\n",
    "pickle.dump(knn, open('models/curr_models/knn-f2-4.pkl', 'wb'))\n",
    "\n",
    "# pickle.dump(xgb1, open('models/curr_models/xgBoost-f2-4.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = cross_validate(mlp, X, y, scoring=['precision', 'recall', 'f1', 'average_precision'], cv=10)\n",
    "print([(x, np.array(l[x]).mean()) for x in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "cv = em.cross_validate(X.values, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(em, open(\"models/curr_models/em_one.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(knn, open(\"models/curr_models/knn-f3-4.pkl\", \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_precision_score(y_test, em.predict_proba(X_test)[:,1]))\n",
    "print(average_precision_score(y_test, mlp.predict_proba(X_test)[:,1]))\n",
    "print(average_precision_score(y_test, knn.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get testing knn to compare against\n",
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=1, n_jobs=-1), n_estimators=1, n_jobs=-1)\n",
    "knn.fit(X_train.values, y_train)\n",
    "\n",
    "print(\"validating knn\")\n",
    "name = 'knn'\n",
    "x = cross_validate(knn, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision', 'neg_log_loss'])\n",
    "if (name not in model_scores):\n",
    "    model_scores[name] = {}\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "        model_scores[name][k]=v.mean()\n",
    "else:\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "    print('already in model_scores')\n",
    "\n",
    "pickle.dump(knn, open(\"models/curr_models/knn-f2-4.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_scores['xgb'])\n",
    "print(model_scores['knn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validating svm\")\n",
    "name = 'svm'\n",
    "x = cross_validate(temp_svm, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision', 'neg_log_loss'])\n",
    "if (name not in model_scores):\n",
    "    model_scores[name] = {}\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "        model_scores[name][k]=v.mean()\n",
    "else:\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "    print('already in model_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "\n",
    "print(cv.keys())\n",
    "name = 'ensemble'\n",
    "if name not in model_scores:\n",
    "    model_scores[name] = {}\n",
    "    for k, v in cv.items():\n",
    "        if k != 'reg_prec':\n",
    "            print(k, mean(v))\n",
    "            model_scores[name][\"test_\"+ k] = mean(v)\n",
    "        if k == 'log_loss':\n",
    "            model_scores[name][\"test_neg_\"+ k] = -1*mean(v)\n",
    "        else:\n",
    "            model_scores[name][k]= v\n",
    "# print(model_scores)\n",
    "\n",
    "# pickle.dump(cv, open(\"cv.pkl\", \"wb\"))\n",
    "#     # print(x, np.array(cv[x]).mean())\n",
    "# print(len(cv['reg_prec'][0]))\n",
    "# for x in cv['reg_prec']:\n",
    "#     # for each model\n",
    "#     print(np.mean(x))\n",
    "#     # print(np.mean(x[0]), np.mean(x[1]))\n",
    "\n",
    "# knn\n",
    "\n",
    "print(model_scores)\n",
    "\n",
    "# rf\n",
    "print(\"validating rf\")\n",
    "name = 'random forest'\n",
    "x = cross_validate(randforest, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision', 'neg_log_loss'])\n",
    "if (name not in model_scores):\n",
    "    model_scores[name] = {}\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "        model_scores[name][k]=v.mean()\n",
    "else:\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "    print('already in model_scores')\n",
    "\n",
    "\n",
    "# mlp\n",
    "print(\"validating mlp\")\n",
    "name = 'mlp'\n",
    "x = cross_validate(mlp, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision', 'neg_log_loss'])\n",
    "if (name not in model_scores):\n",
    "    model_scores[name] = {}\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "        model_scores[name][k]=v.mean()\n",
    "else:\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "    print('already in model_scores')\n",
    "\n",
    "# # xgb\n",
    "print(\"validating xgb\")\n",
    "name='xgb'\n",
    "x = cross_validate(xgb1, X, y, cv=10, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision', 'neg_log_loss'])\n",
    "if (name not in model_scores):\n",
    "    model_scores[name] = {}\n",
    "    for k, v in x.items():\n",
    "        print(k, v.mean())\n",
    "        model_scores[name][k]=v.mean()\n",
    "# else:\n",
    "#     for k, v in x.items():\n",
    "#         print(k, v.mean())\n",
    "#     print('already in model_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_scores, open(\"model_scores.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X, y):\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize lists to store the precision, recall, and AUC values for each fold\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    # Loop through each fold\n",
    "    print(\"beginning cv\")\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train the model on the training data\n",
    "        if type(model).__name__ == 'StackingClassifier':\n",
    "            model.fit_not_pretrained(X_train, y_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        auc_val = auc(recall, precision)\n",
    "        \n",
    "        # Store the precision, recall, and AUC values for this fold\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        auc_list.append(auc_val)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the AUC across all folds\n",
    "    mean_auc = np.mean(auc_list)\n",
    "    std_auc = np.std(auc_list)\n",
    "    mean_recall = np.mean(recall_list, axis=0)\n",
    "    mean_precision = np.mean(precision_list, axis=0)\n",
    "\n",
    "\n",
    "    # Plot the mean precision-recall curve, along with the standard deviation\n",
    "    plt.plot(mean_recall, mean_precision, color='b', label='Mean AUC = %0.2f $\\pm$ %0.2f' % (mean_auc, std_auc))\n",
    "    # plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_scores.keys())\n",
    "print(model_scores['ensemble'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = pickle.load(open(\"model_scores.pkl\", \"rb\"))\n",
    "print(model_scores.keys())\n",
    "for k, v in model_scores.items():\n",
    "    # graph each model's precision recall curve\n",
    "    print(model_scores['ensemble']['test_average_precision'], v['test_average_precision'])\n",
    "    if k == 'ensemble':\n",
    "        print(k, 'brier', (model_scores['ensemble']['test_neg_brier_score']))\n",
    "        print(k, 'logloss', (model_scores['ensemble']['test_neg_log_loss']))\n",
    "        print(k, 'ap', (model_scores['ensemble']['test_average_precision']))\n",
    "        print(k, 'roc', (model_scores['ensemble']['test_roc_auc']))\n",
    "        print(k, 'f1', (model_scores['ensemble']['test_f1']))\n",
    "        print(k, 'acc', (model_scores['ensemble']['test_accuracy']))\n",
    "        print(k, 'prec', (model_scores['ensemble']['test_precision']))\n",
    "        continue\n",
    "    print(k, 'test_average_precision', (model_scores['ensemble']['test_average_precision'] - v['test_average_precision'])/v['test_average_precision'])\n",
    "    print(k, 'roc', (model_scores['ensemble']['test_roc_auc'] - v['test_roc_auc'])/v['test_roc_auc'])\n",
    "    print(k, 'f1', (model_scores['ensemble']['test_f1'] - v['test_f1'])/v['test_f1'])\n",
    "    print(k, 'acc', (model_scores['ensemble']['test_accuracy'] - v['test_accuracy'])/v['test_accuracy'])\n",
    "    print(k, 'prec', (model_scores['ensemble']['test_precision'] - v['test_precision'])/v['test_precision'])\n",
    "    print(k, 'brier', -(model_scores['ensemble']['test_neg_brier_score'] - v['test_neg_brier_score'])/v['test_neg_brier_score'])\n",
    "    print(k, 'neg loss', -(model_scores['ensemble']['test_neg_log_loss'] - v['test_neg_log_loss'])/v['test_neg_log_loss'])\n",
    "    print(k, 'recall', (model_scores['ensemble']['test_recall'] - v['test_recall'])/v['test_recall'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(model_scores).T\n",
    "df.dtypes\n",
    "for column in df.columns:\n",
    "    try:\n",
    "        df[column] = df[column].astype(float)\n",
    "        print(\"success\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df['PR_AUC'] = df['test_average_precision']\n",
    "df.drop(columns=['test_average_precision'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df[['PR_AUC', 'test_f1']].plot.bar(figsize=(8, 8), ylim=(0.4, 1), alpha=0.5, rot=30, fontsize=20)\n",
    "plt.legend(fontsize = 20)\n",
    "\n",
    "# df[['test_average_precision', 'test_roc_auc', 'test_f1']].plot.bar(y=['test_average_precision', 'test_roc_auc', 'test_f1'], figsize=(8, 8), ylim=(0.4, 1), alpha=0.5)\n",
    "\n",
    "# increase alpha for the second bar plot\n",
    "# df.loc['ensemble'].plot.bar(y=['test_average_precision', 'test_roc_auc', 'test_f1'], figsize=(8,8),ylim=(0.4, 1),alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot precision recall for each model\n",
    "# ds = dataset['f2-4']\n",
    "# X, y = ds['X'], ds['y']\n",
    "# for k, v in model_scores.items():\n",
    "#     # graph each model's precision recall curve\n",
    "#     print(k, v['test_average_precision'])\n",
    "#     print(k, v['test_recall'])\n",
    "#     print(k, v['test_f1'])\n",
    "#     print(k, v['test_accuracy'])\n",
    "#     print(k, v['test_precision'])\n",
    "\n",
    "# print(model_scores['ensemble']['reg_prec'][0][0]) # 0 is recall, 1 is precision\n",
    "\n",
    "# # plot recall first, then precision\n",
    "plt.ylim(0.49, 1.01)\n",
    "aa = pickle.load(open('cv.pkl', 'rb'))['reg_prec']\n",
    "plt.plot(aa[1][0], aa[1][1], marker='.', label='ensemble', color='red', linewidth=1)\n",
    "# print(auc(model_scores['ensemble']['reg_prec'][1][0], model_scores['ensemble']['reg_prec'][1][1]))\n",
    "\n",
    "# plot recall first, then precision\n",
    "\n",
    "# test(em, X, y)\n",
    "\n",
    "# test(knn, X.values, y)\n",
    "\n",
    "# test(randforest, X.values, y)\n",
    "\n",
    "# test(mlp, X.values, y)\n",
    "\n",
    "# test(xgb1, X.values, y)\n",
    "\n",
    "# plot recall first, then precision\n",
    "draw_avg_roc_curve(randforest, \"random forest\", X, y)\n",
    "draw_avg_roc_curve(knn, \"knn\", X, y)\n",
    "\n",
    "\n",
    "# plot recall first, then precision\n",
    "# draw_avg_roc_curve(mlp, \"mlp\", X, y)\n",
    "\n",
    "# plot recall first, then precision\n",
    "# draw_avg_roc_curve(xgb1, \"xgb\", X, y)\n",
    "\n",
    "\n",
    "# no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "# plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill', linewidth=2)\n",
    "# draw_avg_roc_curve(xgb1, \"xgb\", X, y)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Cross-Validated Precision-Recall AUC')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_avg_roc_curve(model, name, X, y, multiple=False):\n",
    "    # done w/ the help of https://stats.stackexchange.com/questions/186337/average-roc-for-repeated-10-fold-cross-validation-with-probability-estimates\n",
    "    # plt.ylim(0.50, 1.01)\n",
    "    splits = 5\n",
    "    kf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "    \n",
    "    avgauc = 0\n",
    "    \n",
    "    max_len_x = train_test_split(X, y, test_size=0.2, random_state=42)[0].shape[0]+1\n",
    "    max_len_y = train_test_split(X, y, test_size=0.2, random_state=42)[2].shape[0]+1\n",
    "\n",
    "    print(\"max len x: \" + str(max_len_x))\n",
    "    print(\"max len y: \" + str(max_len_y))\n",
    "\n",
    "    for train, test in kf.split(X, y):\n",
    "        # y_pred_proba = model.predict_proba(X.iloc[test])[::,1]\n",
    "        # fpr, tpr, _ = roc_curve(y[test], y_pred_proba)\n",
    "        # auc_thing = roc_auc_score(y[test], y_pred_proba)\n",
    "        # print(\"roc: \" + str(auc_thing))\n",
    "        # print(train)\n",
    "        # print(test)\n",
    "        print(len(train), len(test))\n",
    "        # if the length is greater than the max length, then chop off the excess\n",
    "        if len(train) > max_len_x:\n",
    "            train = train[:max_len_x]\n",
    "\n",
    "        if len(test) > max_len_y:\n",
    "            test = test[:max_len_y]\n",
    "\n",
    "        \n",
    "        model = model.fit(X.iloc[train], y[train])\n",
    "        print(\"fit done\")\n",
    "        y_score = model.predict_proba(X.iloc[test])\n",
    "        precision, recall, _ = precision_recall_curve(y[test], y_score[:, 1])\n",
    "        auc_thing = auc(recall, precision)\n",
    "        \n",
    "        # if not multiple:\n",
    "        #     # plot variance\n",
    "        #     plt.plot(recall, precision, alpha=0.15)\n",
    "\n",
    "        avgauc += auc_thing\n",
    "        print(\"auc split: \", auc_thing)\n",
    "\n",
    "        # pad with 0s\n",
    "        print(\"precision len: \", len(precision))\n",
    "        print(\"recall len: \", len(recall))\n",
    "        \n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        \n",
    "    \n",
    "    avgauc /= splits\n",
    "    # recall_scores\n",
    "\n",
    "    precision_scores = np.mean(precision_scores, axis=0)\n",
    "    recall_scores = np.mean(recall_scores, axis=0)\n",
    "\n",
    "\n",
    "    if name.lower() == \"ensemble\":\n",
    "        plt.plot(recall_scores, precision_scores, label=f\"{name}\", color=\"red\")\n",
    "    else:\n",
    "        plt.plot(recall_scores, precision_scores, label=f\"{name}\")\n",
    "    # fill in areas between\n",
    "    \n",
    "    return round(avgauc, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_avg_roc_curve(randforest, \"random forest\", X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model_scores.items():\n",
    "    # print(v)\n",
    "    # print(v.keys())\n",
    "    print(k, v['test_neg_log_loss'])\n",
    "    # print(k, v['test_neg_brier_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(em.meta_classifier)\n",
    "pickle.dump(cv, open('em-score.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(em, open('models/curr_models/em-f2-4.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn researchers\n",
    "ds = dataset['f3-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# knn\n",
    "knn = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=1, n_jobs=-1), n_estimators=1, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, em.predict_proba(X_test)[:,1])\n",
    "area = auc(recall, precision)\n",
    "\n",
    "print('Area Under Curve: %.2f' % area)\n",
    "\n",
    "print(accuracy_score(y_test, em.predict(X_test)))\n",
    "\n",
    "# print(average_precision_score(y_test, em.predict_proba(X_test)[:,1]))\n",
    "\n",
    "# plot precision-recall curve\n",
    "plt.plot(recall, precision, marker='.', label='Stacking', linewidth=2)\n",
    "\n",
    "# get precision recall curve\n",
    "# plot the precision-recall curves\n",
    "print(auc(recall, precision))\n",
    "\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "# plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill', linewidth=2)\n",
    "\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, xgb1.predict_proba(X_test)[:, 1])\n",
    "# plt.plot(recall, precision, marker='.', label='XGBoost')\n",
    "\n",
    "\n",
    "# plot precision recall for knn\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, randforest.predict_proba(X_test)[:, 1])\n",
    "plt.plot(recall, precision, marker='.', label='Random Forest', linewidth=2)\n",
    "print(auc(recall, precision))\n",
    "\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, mlp.predict_proba(X_test)[:, 1])\n",
    "# plt.plot(recall, precision, marker='.', label='MLP', linewidth=2)\n",
    "# print(auc(recall, precision))\n",
    "ds = dataset['f3-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, knn.predict_proba(X_test)[:, 1])\n",
    "plt.xlim([0.49, 1.01])\n",
    "plt.ylim([0.49, 1.01])\n",
    "plt.plot(recall, precision, marker='.', label='KNN', linewidth=2)\n",
    "print(auc(recall, precision))\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn, random forest, xgboost, mlp, svm, gradient boosting classifier, logistic regression\n",
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, em.predict_proba(X_test)[:,1])\n",
    "area = auc(recall, precision)\n",
    "\n",
    "print('Area Under Curve: %.2f' % area)\n",
    "\n",
    "print(accuracy_score(y_test, em.predict(X_test)))\n",
    "\n",
    "# pickle.dump(em, open('models/curr_models/custom-ensemble-f2-4.pkl', 'wb'))\n",
    "# print(average_precision_score(y_test, em.predict_proba(X_test)[:,1]))\n",
    "\n",
    "# plot precision-recall curve\n",
    "plt.plot(recall, precision, marker='.', label='Stacking', linewidth=2)\n",
    "\n",
    "# get precision recall curve\n",
    "# plot the precision-recall curves\n",
    "print(auc(recall, precision))\n",
    "\n",
    "# no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "\n",
    "# plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill', linewidth=2)\n",
    "\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, xgb1.predict_proba(X_test)[:, 1])\n",
    "# plt.plot(recall, precision, marker='.', label='XGBoost')\n",
    "\n",
    "\n",
    "# plot precision recall for knn\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, randforest.predict_proba(X_test)[:, 1])\n",
    "plt.plot(recall, precision, marker='.', label='Random Forest', linewidth=2)\n",
    "print(auc(recall, precision))\n",
    "\n",
    "# precision, recall, thresholds = precision_recall_curve(y_test, mlp.predict_proba(X_test)[:, 1])\n",
    "# plt.plot(recall, precision, marker='.', label='MLP', linewidth=2)\n",
    "# print(auc(recall, precision))\n",
    "ds = dataset['f3-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, knn.predict_proba(X_test)[:, 1])\n",
    "plt.xlim([0.49, 1.01])\n",
    "plt.ylim([0.49, 1.01])\n",
    "plt.plot(recall, precision, marker='.', label='KNN', linewidth=2)\n",
    "print(auc(recall, precision))\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset['f2-4']\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, em.predict_proba(X_test)[:,1])\n",
    "auc_thing = roc_auc_score(y_test, em.predict_proba(X_test)[:,1])\n",
    "print(\"roc: \" + str(auc_thing))\n",
    "plt.plot(fpr,tpr, marker='.', label='Stacking')\n",
    "\n",
    "# AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, randforest.predict_proba(X_test)[:,1])\n",
    "auc_thing = roc_auc_score(y_test, randforest.predict_proba(X_test)[:,1])\n",
    "print(\"roc: \" + str(auc_thing))\n",
    "plt.plot(fpr,tpr, marker='.', label='Random Forest')\n",
    "\n",
    "# AUC\n",
    "# fpr, tpr, _ = roc_curve(y_test, mlp.predict_proba(X_test)[:,1])\n",
    "# auc_thing = roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1])\n",
    "# print(\"roc: \" + str(auc_thing))\n",
    "# plt.plot(fpr,tpr, marker='.', label='MLP')\n",
    "\n",
    "# # AUC\n",
    "# fpr, tpr, _ = roc_curve(y_test, xgb1.predict_proba(X_test)[:,1])\n",
    "# auc_thing = roc_auc_score(y_test, xgb1.predict_proba(X_test)[:,1])\n",
    "# print(\"roc: \" + str(auc_thing))\n",
    "# plt.plot(fpr,tpr, marker='.', label='XGBoost')\n",
    "\n",
    "# AUC\n",
    "ds = dataset['f3-4'] # researchers\n",
    "X, y = ds['X'], ds['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, knn.predict_proba(X_test)[:,1])\n",
    "auc_thing = roc_auc_score(y_test, knn.predict_proba(X_test)[:,1])\n",
    "print(\"roc: \" + str(auc_thing))\n",
    "plt.plot(fpr,tpr, marker='.', label='KNN')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brier_score_loss(y_test, em.predict_proba(X_test)[:,1]))\n",
    "print(brier_score_loss(y_test, randforest.predict_proba(X_test)[:,1]))\n",
    "print(brier_score_loss(y_test, knn.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(log_loss(y_test, em.predict_proba(X_test)[:,1]))\n",
    "print(log_loss(y_test, randforest.predict_proba(X_test)[:,1]))\n",
    "print(log_loss(y_test, knn.predict_proba(X_test)[:,1]))\n",
    "print(log_loss(y_test, mlp.predict_proba(X_test)[:,1]))\n",
    "print(log_loss(y_test, xgb1.predict_proba(X_test)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cross_validate(em.meta_clf_, X, y, cv=5, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score', 'average_precision'], verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in x.items():\n",
    "    print(k, v.mean())\n",
    "    # modelScores[name][k]=v.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = cross_val_score(em, X, y, cv=5, scoring='average_precision', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, em.predict(X_test)))\n",
    "print(recall_score(y_test, em.predict(X_test)))\n",
    "print(f1_score(y_test, em.predict(X_test)))\n",
    "# pickle.dump(em, open('models/curr_models/ensemble.pkl', 'wb'))\n",
    "asdf = pickle.load(open('models/curr_models/xgb1-test.pkl', 'rb'))\n",
    "\n",
    "\n",
    "# print(em.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, asdf.predict(X_test)))\n",
    "print(recall_score(y_test, asdf.predict(X_test)))\n",
    "print(f1_score(y_test, asdf.predict(X_test)))\n",
    "\n",
    "print(accuracy_score(y_test, temp_svm.predict(X_test)))\n",
    "print(recall_score(y_test, temp_svm.predict(X_test)))\n",
    "print(f1_score(y_test, temp_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(em.clfs_)\n",
    "x = cross_val_score(em, X, y, cv=2, scoring='recall', verbose=1, n_jobs=-1)\n",
    "# x = cross_validate(em.clfs_, X, y, cv=2, scoring=['recall', 'f1', 'accuracy', 'precision', 'roc_auc', 'neg_brier_score'], verbose=1, n_jobs=-1)\n",
    "# name = 'ensemble_lengthdiv_4'\n",
    "# if (name not in modelScores):\n",
    "#     modelScores[name] = {}\n",
    "#     for k, v in x.items():\n",
    "#         print(k, v.mean())\n",
    "#         modelScores[name][k]=v.mean()\n",
    "# else:\n",
    "#     print('already in modelScores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump em\n",
    "pickle.dump(em, open('models/curr_models/em.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('seq')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1360c63304de9435a2a3572d38e6c9496b6fb5d1617f35fbc8638664d664ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
